"""
Client Service - Consumes IDs from multiple ID generator instances.

This service demonstrates how clients can interact with multiple ID generators
using round-robin or random selection, and includes collision detection.
"""
import asyncio
import logging
import os
import random
import time
from collections import defaultdict
from contextlib import asynccontextmanager
from dataclasses import dataclass
from datetime import datetime
from typing import Dict, List, Optional, Set

import httpx
from fastapi import FastAPI, HTTPException
from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.instrumentation.httpx import HTTPXClientInstrumentor
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from prometheus_client import Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST
from starlette.responses import Response

# Configuration
SERVICE_NAME = os.getenv("OTEL_SERVICE_NAME", "client")
OTEL_ENDPOINT = os.getenv("OTEL_EXPORTER_OTLP_ENDPOINT", "http://otel-collector:4317")
GENERATOR_URLS = os.getenv("GENERATOR_URLS", "http://id-generator-1:8001").split(",")
PORT = int(os.getenv("PORT", "8000"))

# Logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(SERVICE_NAME)

# OpenTelemetry setup
resource = Resource.create({"service.name": SERVICE_NAME})
provider = TracerProvider(resource=resource)
processor = BatchSpanProcessor(OTLPSpanExporter(endpoint=OTEL_ENDPOINT, insecure=True))
provider.add_span_processor(processor)
trace.set_tracer_provider(provider)
tracer = trace.get_tracer(__name__)

# Instrument httpx for trace propagation
HTTPXClientInstrumentor().instrument()

# Prometheus metrics
IDS_REQUESTED = Counter(
    "client_ids_requested_total",
    "Total IDs requested from generators",
    ["generator"]
)
IDS_RECEIVED = Counter(
    "client_ids_received_total",
    "Total IDs successfully received",
    ["generator"]
)
REQUEST_ERRORS = Counter(
    "client_request_errors_total",
    "Total request errors",
    ["generator", "error_type"]
)
COLLISIONS_DETECTED = Counter(
    "client_collisions_detected_total",
    "Total ID collisions detected"
)
REQUEST_LATENCY = Histogram(
    "client_request_duration_seconds",
    "Latency of ID requests",
    ["generator"],
    buckets=[0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0]
)
UNIQUE_IDS_SEEN = Gauge(
    "client_unique_ids_seen_total",
    "Total unique IDs seen (for collision detection)"
)


class CollisionDetector:
    """Tracks generated IDs to detect collisions"""

    def __init__(self, max_tracked: int = 100000):
        self.max_tracked = max_tracked
        self._seen_ids: Set[int] = set()
        self._id_sources: Dict[int, str] = {}  # id -> generator
        self._collision_count = 0

    def check_and_add(self, snowflake_id: int, generator: str) -> bool:
        """
        Check if ID was seen before and add it.
        Returns True if collision detected.
        """
        if snowflake_id in self._seen_ids:
            self._collision_count += 1
            COLLISIONS_DETECTED.inc()
            original_source = self._id_sources.get(snowflake_id, "unknown")
            logger.error(
                f"COLLISION DETECTED! ID {snowflake_id} generated by both "
                f"{original_source} and {generator}"
            )
            return True

        self._seen_ids.add(snowflake_id)
        self._id_sources[snowflake_id] = generator

        # Cleanup old IDs if too many
        if len(self._seen_ids) > self.max_tracked:
            # Remove oldest 10%
            to_remove = list(self._seen_ids)[:self.max_tracked // 10]
            for id_to_remove in to_remove:
                self._seen_ids.discard(id_to_remove)
                self._id_sources.pop(id_to_remove, None)

        UNIQUE_IDS_SEEN.set(len(self._seen_ids))
        return False

    def get_stats(self) -> dict:
        return {
            "unique_ids_tracked": len(self._seen_ids),
            "collision_count": self._collision_count,
            "max_tracked": self.max_tracked
        }

    def clear(self):
        self._seen_ids.clear()
        self._id_sources.clear()
        self._collision_count = 0
        UNIQUE_IDS_SEEN.set(0)


# Global instances
collision_detector = CollisionDetector()
generator_index = 0  # For round-robin


def get_next_generator() -> str:
    """Round-robin generator selection"""
    global generator_index
    generator = GENERATOR_URLS[generator_index % len(GENERATOR_URLS)]
    generator_index += 1
    return generator


@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info(f"{SERVICE_NAME} starting up")
    logger.info(f"Generator URLs: {GENERATOR_URLS}")
    yield
    logger.info(f"{SERVICE_NAME} shutting down")


app = FastAPI(title=SERVICE_NAME, lifespan=lifespan)
FastAPIInstrumentor.instrument_app(app)


@app.get("/health")
async def health():
    return {
        "status": "ok",
        "service": SERVICE_NAME,
        "generators": GENERATOR_URLS
    }


@app.get("/metrics")
async def metrics():
    return Response(content=generate_latest(), media_type=CONTENT_TYPE_LATEST)


@app.get("/id")
async def get_id(strategy: str = "roundrobin"):
    """
    Get a single ID from a generator.

    strategy: 'roundrobin' or 'random'
    """
    current_span = trace.get_current_span()

    if strategy == "random":
        generator_url = random.choice(GENERATOR_URLS)
    else:
        generator_url = get_next_generator()

    current_span.set_attribute("generator.url", generator_url)
    IDS_REQUESTED.labels(generator=generator_url).inc()

    start_time = time.time()

    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get(f"{generator_url}/generate")
            response.raise_for_status()
            data = response.json()

        duration = time.time() - start_time
        REQUEST_LATENCY.labels(generator=generator_url).observe(duration)
        IDS_RECEIVED.labels(generator=generator_url).inc()

        snowflake_id = data["id"]

        # Check for collisions
        collision = collision_detector.check_and_add(snowflake_id, generator_url)

        current_span.set_attribute("snowflake.id", str(snowflake_id))
        current_span.set_attribute("collision.detected", collision)

        return {
            "id": snowflake_id,
            "id_str": str(snowflake_id),
            "generator": generator_url,
            "worker_id": data.get("worker_id"),
            "datacenter_id": data.get("datacenter_id"),
            "collision_detected": collision,
            "request_time_ms": round(duration * 1000, 2)
        }

    except httpx.TimeoutException:
        REQUEST_ERRORS.labels(generator=generator_url, error_type="timeout").inc()
        current_span.set_attribute("error", True)
        raise HTTPException(status_code=504, detail=f"Timeout from {generator_url}")
    except httpx.HTTPError as e:
        REQUEST_ERRORS.labels(generator=generator_url, error_type="http_error").inc()
        current_span.set_attribute("error", True)
        raise HTTPException(status_code=502, detail=f"Error from {generator_url}: {str(e)}")


@app.get("/ids/batch")
async def get_batch_ids(count: int = 10, strategy: str = "roundrobin"):
    """Get multiple IDs, distributing across generators"""
    if count < 1 or count > 100:
        raise HTTPException(status_code=400, detail="Count must be between 1 and 100")

    current_span = trace.get_current_span()
    current_span.set_attribute("batch.size", count)

    ids = []
    errors = []
    collisions = 0
    start_time = time.time()

    async with httpx.AsyncClient(timeout=5.0) as client:
        for i in range(count):
            if strategy == "random":
                generator_url = random.choice(GENERATOR_URLS)
            else:
                generator_url = get_next_generator()

            try:
                IDS_REQUESTED.labels(generator=generator_url).inc()
                response = await client.get(f"{generator_url}/generate")
                response.raise_for_status()
                data = response.json()

                IDS_RECEIVED.labels(generator=generator_url).inc()

                snowflake_id = data["id"]
                collision = collision_detector.check_and_add(snowflake_id, generator_url)
                if collision:
                    collisions += 1

                ids.append({
                    "id": snowflake_id,
                    "generator": generator_url,
                    "worker_id": data.get("worker_id"),
                    "collision": collision
                })

            except Exception as e:
                errors.append({
                    "generator": generator_url,
                    "error": str(e)
                })
                REQUEST_ERRORS.labels(generator=generator_url, error_type="batch_error").inc()

    duration = time.time() - start_time

    return {
        "ids": ids,
        "count_requested": count,
        "count_received": len(ids),
        "errors": errors,
        "collisions_detected": collisions,
        "total_time_ms": round(duration * 1000, 2),
        "avg_time_per_id_ms": round((duration * 1000) / max(len(ids), 1), 2)
    }


@app.get("/ids/parallel")
async def get_parallel_ids(count: int = 10):
    """Get IDs from all generators in parallel"""
    if count < 1 or count > 100:
        raise HTTPException(status_code=400, detail="Count must be between 1 and 100")

    current_span = trace.get_current_span()
    current_span.set_attribute("parallel.count_per_generator", count)

    start_time = time.time()

    async def fetch_from_generator(generator_url: str, n: int) -> dict:
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.post(
                    f"{generator_url}/generate/batch",
                    params={"count": n}
                )
                response.raise_for_status()
                data = response.json()

                IDS_RECEIVED.labels(generator=generator_url).inc(n)

                return {
                    "generator": generator_url,
                    "ids": data.get("ids", []),
                    "worker_id": data.get("worker_id"),
                    "success": True
                }
        except Exception as e:
            REQUEST_ERRORS.labels(generator=generator_url, error_type="parallel_error").inc()
            return {
                "generator": generator_url,
                "ids": [],
                "error": str(e),
                "success": False
            }

    # Fetch from all generators in parallel
    tasks = [fetch_from_generator(url, count) for url in GENERATOR_URLS]
    results = await asyncio.gather(*tasks)

    # Aggregate results and check for collisions
    all_ids = []
    collisions = 0
    for result in results:
        for snowflake_id in result.get("ids", []):
            collision = collision_detector.check_and_add(snowflake_id, result["generator"])
            if collision:
                collisions += 1
            all_ids.append({
                "id": snowflake_id,
                "generator": result["generator"],
                "collision": collision
            })

    duration = time.time() - start_time

    return {
        "results": results,
        "all_ids": all_ids,
        "total_ids": len(all_ids),
        "collisions_detected": collisions,
        "generators_used": len(GENERATOR_URLS),
        "total_time_ms": round(duration * 1000, 2)
    }


@app.get("/parse/{snowflake_id}")
async def parse_id(snowflake_id: int):
    """Parse a Snowflake ID (proxy to any generator)"""
    generator_url = GENERATOR_URLS[0]

    async with httpx.AsyncClient(timeout=5.0) as client:
        response = await client.get(f"{generator_url}/parse/{snowflake_id}")
        return response.json()


@app.get("/collision-stats")
async def collision_stats():
    """Get collision detection statistics"""
    return collision_detector.get_stats()


@app.post("/collision-stats/reset")
async def reset_collision_stats():
    """Reset collision detection statistics"""
    collision_detector.clear()
    return {"status": "reset", "stats": collision_detector.get_stats()}


@app.get("/generators")
async def list_generators():
    """List all configured generators and their status"""
    results = []

    async with httpx.AsyncClient(timeout=3.0) as client:
        for url in GENERATOR_URLS:
            try:
                response = await client.get(f"{url}/info")
                if response.status_code == 200:
                    data = response.json()
                    results.append({
                        "url": url,
                        "status": "healthy",
                        "worker_id": data.get("worker_id"),
                        "datacenter_id": data.get("datacenter_id"),
                        "instance_name": data.get("instance_name")
                    })
                else:
                    results.append({
                        "url": url,
                        "status": "unhealthy",
                        "error": f"HTTP {response.status_code}"
                    })
            except Exception as e:
                results.append({
                    "url": url,
                    "status": "unreachable",
                    "error": str(e)
                })

    return {
        "generators": results,
        "total": len(GENERATOR_URLS),
        "healthy": sum(1 for r in results if r["status"] == "healthy")
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=PORT)
