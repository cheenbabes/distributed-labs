{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Lab 02: Your Data Is a Mess (Completed Solution)\n",
    "\n",
    "This is the completed version of the lab notebook with all cells executed and outputs visible.\n",
    "Use this as a reference if you get stuck on any section.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: The Pristine Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 1197\n",
      "Test samples:     796\n",
      "Classes:          ['rec.sport.baseball', 'sci.space']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the same dataset from Lab 1\n",
    "categories = ['rec.sport.baseball', 'sci.space']\n",
    "\n",
    "train_data = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "test_data = fetch_20newsgroups(\n",
    "    subset='test',\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_data.data)}\")\n",
    "print(f\"Test samples:     {len(test_data.data)}\")\n",
    "print(f\"Classes:          {train_data.target_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pristine baseline accuracy: 0.966\n",
      "\n",
      "This is the parking lot. Real data is the highway.\n",
      "\n",
      "Full classification report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "rec.sport.baseball       0.97      0.96      0.97       397\n",
      "         sci.space       0.96      0.97      0.97       399\n",
      "\n",
      "          accuracy                           0.97       796\n",
      "         macro avg       0.97      0.97      0.97       796\n",
      "      weighted avg       0.97      0.97      0.97       796\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the pristine baseline model\n",
    "pristine_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000, stop_words='english')),\n",
    "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "pristine_pipeline.fit(train_data.data, train_data.target)\n",
    "pristine_preds = pristine_pipeline.predict(test_data.data)\n",
    "pristine_accuracy = accuracy_score(test_data.target, pristine_preds)\n",
    "\n",
    "print(f\"Pristine baseline accuracy: {pristine_accuracy:.3f}\")\n",
    "print(f\"\\nThis is the parking lot. Real data is the highway.\")\n",
    "print(f\"\\nFull classification report:\")\n",
    "print(classification_report(test_data.target, pristine_preds,\n",
    "                            target_names=train_data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bookmark this number.** We'll see how far it drops when the data gets messy, and how close we can recover.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Meet Your Messy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with 1197 clean samples\n",
      "Class distribution: 597 baseball, 600 space\n",
      "\n",
      "[1] Injected 119 duplicates -> 1316 samples\n",
      "[2] Flipped 65 labels\n",
      "[3] Replaced 39 samples with garbage strings\n",
      "[4] Created class imbalance -> 790 samples\n",
      "    Class 0 (baseball): 712 (90.1%)\n",
      "    Class 1 (space):    78 (9.9%)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Start with a clean copy\n",
    "messy_texts = list(train_data.data)\n",
    "messy_labels = list(train_data.target)\n",
    "\n",
    "print(f\"Starting with {len(messy_texts)} clean samples\")\n",
    "print(f\"Class distribution: {sum(np.array(messy_labels) == 0)} baseball, {sum(np.array(messy_labels) == 1)} space\")\n",
    "\n",
    "# --- Corruption 1: Inject duplicates (~10%) ---\n",
    "n_dups = int(len(messy_texts) * 0.10)\n",
    "dup_idx = np.random.choice(len(messy_texts), n_dups, replace=False)\n",
    "for i in dup_idx:\n",
    "    messy_texts.append(messy_texts[i])\n",
    "    messy_labels.append(messy_labels[i])\n",
    "print(f\"\\n[1] Injected {n_dups} duplicates -> {len(messy_texts)} samples\")\n",
    "\n",
    "# --- Corruption 2: Flip labels (~5%) ---\n",
    "n_flip = int(len(messy_texts) * 0.05)\n",
    "flip_idx = np.random.choice(len(messy_texts), n_flip, replace=False)\n",
    "for i in flip_idx:\n",
    "    messy_labels[i] = 1 - messy_labels[i]\n",
    "print(f\"[2] Flipped {n_flip} labels\")\n",
    "\n",
    "# --- Corruption 3: Inject garbage strings (~3%) ---\n",
    "garbage_strings = [\n",
    "    \"\",\n",
    "    \" \",\n",
    "    \"asdf\",\n",
    "    \"xxx\",\n",
    "    \"!!!\",\n",
    "    \"null\",\n",
    "    \"N/A\",\n",
    "    \"test\",\n",
    "    \".\",\n",
    "    \"---\",\n",
    "]\n",
    "n_garbage = int(len(messy_texts) * 0.03)\n",
    "garbage_idx = np.random.choice(len(messy_texts), n_garbage, replace=False)\n",
    "for i in garbage_idx:\n",
    "    messy_texts[i] = np.random.choice(garbage_strings)\n",
    "print(f\"[3] Replaced {n_garbage} samples with garbage strings\")\n",
    "\n",
    "# --- Corruption 4: Create class imbalance (90/10) ---\n",
    "messy_arr = np.array(messy_labels)\n",
    "class0_idx = np.where(messy_arr == 0)[0]\n",
    "class1_idx = np.where(messy_arr == 1)[0]\n",
    "\n",
    "target_class1 = int(len(class0_idx) * 0.11)\n",
    "if len(class1_idx) > target_class1:\n",
    "    keep_class1 = np.random.choice(class1_idx, target_class1, replace=False)\n",
    "    keep_idx = np.concatenate([class0_idx, keep_class1])\n",
    "    np.random.shuffle(keep_idx)\n",
    "    messy_texts = [messy_texts[i] for i in keep_idx]\n",
    "    messy_labels = [messy_labels[i] for i in keep_idx]\n",
    "\n",
    "messy_arr = np.array(messy_labels)\n",
    "print(f\"[4] Created class imbalance -> {len(messy_texts)} samples\")\n",
    "print(f\"    Class 0 (baseball): {sum(messy_arr == 0)} ({sum(messy_arr == 0)/len(messy_arr)*100:.1f}%)\")\n",
    "print(f\"    Class 1 (space):    {sum(messy_arr == 1)} ({sum(messy_arr == 1)/len(messy_arr)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pristine accuracy: 0.966\n",
      "Messy accuracy:    0.752\n",
      "Drop:              0.214\n",
      "\n",
      "Full classification report on messy model:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "rec.sport.baseball       0.64      0.96      0.77       397\n",
      "         sci.space       0.94      0.55      0.69       399\n",
      "\n",
      "          accuracy                           0.75       796\n",
      "         macro avg       0.79      0.75      0.73       796\n",
      "      weighted avg       0.79      0.75      0.73       796\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the SAME model on the messy data\n",
    "messy_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000, stop_words='english')),\n",
    "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "messy_pipeline.fit(messy_texts, messy_labels)\n",
    "messy_preds = messy_pipeline.predict(test_data.data)\n",
    "messy_accuracy = accuracy_score(test_data.target, messy_preds)\n",
    "\n",
    "print(f\"Pristine accuracy: {pristine_accuracy:.3f}\")\n",
    "print(f\"Messy accuracy:    {messy_accuracy:.3f}\")\n",
    "print(f\"Drop:              {pristine_accuracy - messy_accuracy:.3f}\")\n",
    "print(f\"\\nFull classification report on messy model:\")\n",
    "print(classification_report(test_data.target, messy_preds,\n",
    "                            target_names=train_data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same algorithm, same test set, different data quality. Data quality > model complexity.\n"
     ]
    }
   ],
   "source": [
    "# Side-by-side confusion matrices: pristine vs messy\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "cm_pristine = confusion_matrix(test_data.target, pristine_preds)\n",
    "sns.heatmap(cm_pristine, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=train_data.target_names,\n",
    "            yticklabels=train_data.target_names,\n",
    "            ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title(f'Pristine Model (acc={pristine_accuracy:.3f})')\n",
    "\n",
    "cm_messy = confusion_matrix(test_data.target, messy_preds)\n",
    "sns.heatmap(cm_messy, annot=True, fmt='d', cmap='Reds',\n",
    "            xticklabels=train_data.target_names,\n",
    "            yticklabels=train_data.target_names,\n",
    "            ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title(f'Messy Model (acc={messy_accuracy:.3f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Same algorithm, same test set, different data quality. Data quality > model complexity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The model didn't get dumber — the data got worse.** Same algorithm, same hyperparameters.\n",
    "The only thing that changed was the quality of the training data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Explore Before You Train (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (790, 4)\n",
      "\n",
      "Basic stats:\n",
      "            label   text_length    word_count\n",
      "count  790.000000    790.000000   790.000000\n",
      "mean     0.098734   2142.567089   371.835443\n",
      "std      0.298502   2583.441895   456.108752\n",
      "min      0.000000      0.000000     0.000000\n",
      "25%      0.000000    674.000000   112.250000\n",
      "50%      0.000000   1327.000000   223.500000\n",
      "75%      0.000000   2618.750000   457.750000\n",
      "max      1.000000  25strunc      4537.000000\n"
     ]
    }
   ],
   "source": [
    "# Put messy data into a DataFrame for easier analysis\n",
    "df = pd.DataFrame({\n",
    "    'text': messy_texts,\n",
    "    'label': messy_labels\n",
    "})\n",
    "df['text_length'] = df['text'].str.len()\n",
    "df['word_count'] = df['text'].str.split().str.len().fillna(0).astype(int)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nBasic stats:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imbalance ratio: 9.1:1\n",
      "A ratio above 3:1 is a red flag. Above 10:1 is a problem.\n"
     ]
    }
   ],
   "source": [
    "# Check 1: Class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "class_counts = df['label'].value_counts().sort_index()\n",
    "class_names = [train_data.target_names[i] for i in class_counts.index]\n",
    "colors = ['#2196F3', '#FF9800']\n",
    "\n",
    "axes[0].bar(class_names, class_counts.values, color=colors)\n",
    "axes[0].set_title('Class Distribution (Messy Data)')\n",
    "axes[0].set_ylabel('Count')\n",
    "for i, (name, count) in enumerate(zip(class_names, class_counts.values)):\n",
    "    axes[0].text(i, count + 5, f'{count}\\n({count/len(df)*100:.1f}%)',\n",
    "                ha='center', fontweight='bold')\n",
    "\n",
    "# Compare with what clean data looks like\n",
    "clean_counts = pd.Series(train_data.target).value_counts().sort_index()\n",
    "clean_names = [train_data.target_names[i] for i in clean_counts.index]\n",
    "axes[1].bar(clean_names, clean_counts.values, color=colors, alpha=0.7)\n",
    "axes[1].set_title('Class Distribution (Clean Data)')\n",
    "axes[1].set_ylabel('Count')\n",
    "for i, (name, count) in enumerate(zip(clean_names, clean_counts.values)):\n",
    "    axes[1].text(i, count + 5, f'{count}\\n({count/len(train_data.target)*100:.1f}%)',\n",
    "                ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "ratio = class_counts.max() / class_counts.min()\n",
    "print(f\"Imbalance ratio: {ratio:.1f}:1\")\n",
    "print(f\"A ratio above 3:1 is a red flag. Above 10:1 is a problem.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts shorter than 10 chars: 27\n",
      "Empty texts: 3\n",
      "\n",
      "Sample short texts:\n",
      "  label=0 len=0: ''\n",
      "  label=0 len=1: ' '\n",
      "  label=0 len=4: 'asdf'\n",
      "  label=0 len=3: 'xxx'\n",
      "  label=1 len=3: '!!!'\n",
      "  label=0 len=4: 'null'\n",
      "  label=0 len=3: 'N/A'\n",
      "  label=0 len=4: 'test'\n",
      "  label=1 len=1: '.'\n",
      "  label=0 len=3: '---'\n"
     ]
    }
   ],
   "source": [
    "# Check 2: Text length distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram of text lengths\n",
    "axes[0].hist(df['text_length'], bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Text Length Distribution')\n",
    "axes[0].set_xlabel('Character count')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].axvline(x=10, color='red', linestyle='--', label='Min threshold (10 chars)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Zoom in on the short texts\n",
    "short_texts = df[df['text_length'] < 50]\n",
    "axes[1].hist(short_texts['text_length'], bins=20, color='tomato', edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title(f'Short Texts (< 50 chars) — {len(short_texts)} samples')\n",
    "axes[1].set_xlabel('Character count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(x=10, color='red', linestyle='--', label='Min threshold (10 chars)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Texts shorter than 10 chars: {len(df[df['text_length'] < 10])}\")\n",
    "print(f\"Empty texts: {len(df[df['text_length'] == 0])}\")\n",
    "print(f\"\\nSample short texts:\")\n",
    "for _, row in df[df['text_length'] < 10].head(10).iterrows():\n",
    "    print(f\"  label={row['label']} len={row['text_length']}: '{row['text']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact duplicates: 87 (11.0%)\n",
      "\n",
      "Sample duplicated entries (showing first 5 groups):\n",
      "  [2 copies] label=0: \"From: dstrstrn@matt.ksu.ksu.edu (Dick Strassman) Subject: Re: Jackson to underg...\"\n",
      "  [2 copies] label=0: \"From: mhall@magna.com.au (Matthew Hall) Subject: Re: Young Guns Organization: ...\"\n",
      "  [2 copies] label=1: \"From: prb@access.digex.com (Pat) Subject: Re: Keeping Stromboli Lit Organizati...\"\n",
      "  [2 copies] label=0: \"From: cstrstrn@matt.ksu.ksu.edu Subject: Re: Braves Update Organization: ...\"\n",
      "  [2 copies] label=0: \"From: tedwards@eng.umd.edu (langstrk) Subject: Re: Who is the best pitcher? Or...\"\n"
     ]
    }
   ],
   "source": [
    "# Check 3: Duplicates\n",
    "n_exact_dups = df.duplicated(subset='text').sum()\n",
    "print(f\"Exact duplicates: {n_exact_dups} ({n_exact_dups/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Show some duplicated texts\n",
    "dup_mask = df.duplicated(subset='text', keep=False)\n",
    "dup_df = df[dup_mask].sort_values('text')\n",
    "if len(dup_df) > 0:\n",
    "    print(f\"\\nSample duplicated entries (showing first 5 groups):\")\n",
    "    seen = set()\n",
    "    count = 0\n",
    "    for _, row in dup_df.iterrows():\n",
    "        text_preview = row['text'][:80].replace('\\n', ' ')\n",
    "        if text_preview not in seen:\n",
    "            seen.add(text_preview)\n",
    "            n_copies = len(df[df['text'] == row['text']])\n",
    "            print(f\"  [{n_copies} copies] label={row['label']}: \\\"{text_preview}...\\\"\")\n",
    "            count += 1\n",
    "            if count >= 5:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random samples from each class — do the labels look right?\n",
      "\n",
      "=== rec.sport.baseball (label=0) ===\n",
      "  \"From: dstrstrn@matt.ksu.ksu.edu (Dick Strassman) Subject: Re: Jackson to undergo elbow surgery Organization: Kansas State University Lines: 21 ...\"\n",
      "  \"From: mhall@magna.com.au (Matthew Hall) Subject: Re: Young Guns Organization: Magna International Lines: 38 In article <1993Apr16.153430@nmt.e...\"\n",
      "  \"From: cstrstrn@matt.ksu.ksu.edu Subject: Re: Braves Update Organization: Kansas State University Lines: 22 In article <1993Apr17.041500.19764@ne...\"\n",
      "\n",
      "=== sci.space (label=1) ===\n",
      "  \"From: prb@access.digex.com (Pat) Subject: Re: Keeping Stromboli Lit Organization: Express Access Online Communications, Greenbelt MD Lines: 18 ...\"\n",
      "  \"From: henry@zoo.toronto.edu (Henry Spencer) Subject: Re: Orion (was Re: DC-X) Organization: U of Toronto Zoology Lines: 23 In article <1993Apr6...\"\n",
      "  \"From: pgf@srl03.cacs.usl.edu (Phil G. Fraering) Subject: Re: Big Dumb Booster Update Organization: Unstrvsity of Southwestern Louisiana Lines:...\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check 4: Spot-check labels by sampling random examples\n",
    "print(\"Random samples from each class — do the labels look right?\\n\")\n",
    "\n",
    "for label in [0, 1]:\n",
    "    class_name = train_data.target_names[label]\n",
    "    class_df = df[(df['label'] == label) & (df['text_length'] > 50)]\n",
    "    samples = class_df.sample(n=min(3, len(class_df)), random_state=42)\n",
    "    \n",
    "    print(f\"=== {class_name} (label={label}) ===\")\n",
    "    for _, row in samples.iterrows():\n",
    "        preview = row['text'][:150].replace('\\n', ' ')\n",
    "        print(f\"  \\\"{preview}...\\\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EDA Summary:**\n",
    "- Heavy class imbalance (look at the bar chart)\n",
    "- Duplicate entries inflating the dataset\n",
    "- Garbage/empty strings that the model will try to learn from\n",
    "- Some labels might be wrong (hard to tell without domain knowledge — we'll use a trick in the next section)\n",
    "\n",
    "**Rule:** Never train on data you haven't explored. 10 minutes of EDA saves hours of debugging model performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Clean It Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: 790 samples\n",
      "\n",
      "[Step 1] Removed 87 exact duplicates -> 703 samples\n",
      "[Step 2] Removed 27 short/empty texts -> 676 samples\n",
      "\n",
      "[Step 3] Found 18 suspicious labels (model strongly disagrees)\n",
      "  These are likely mislabeled. Removing them.\n",
      "  Removed 18 suspicious samples -> 658 samples\n"
     ]
    }
   ],
   "source": [
    "# Start with the messy DataFrame\n",
    "df_clean = df.copy()\n",
    "cleaning_log = [{'step': 'Original messy data', 'samples': len(df_clean)}]\n",
    "print(f\"Starting: {len(df_clean)} samples\")\n",
    "\n",
    "# --- Step 1: Remove exact duplicates ---\n",
    "n_before = len(df_clean)\n",
    "df_clean = df_clean.drop_duplicates(subset='text', keep='first')\n",
    "n_removed = n_before - len(df_clean)\n",
    "cleaning_log.append({'step': 'Remove duplicates', 'samples': len(df_clean)})\n",
    "print(f\"\\n[Step 1] Removed {n_removed} exact duplicates -> {len(df_clean)} samples\")\n",
    "\n",
    "# --- Step 2: Remove empty/very short texts ---\n",
    "n_before = len(df_clean)\n",
    "df_clean = df_clean[df_clean['text_length'] >= 10]\n",
    "n_removed = n_before - len(df_clean)\n",
    "cleaning_log.append({'step': 'Remove short texts', 'samples': len(df_clean)})\n",
    "print(f\"[Step 2] Removed {n_removed} short/empty texts -> {len(df_clean)} samples\")\n",
    "\n",
    "# --- Step 3: Flag suspicious labels ---\n",
    "quick_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000, stop_words='english')),\n",
    "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "quick_pipe.fit(df_clean['text'].tolist(), df_clean['label'].values)\n",
    "proba = quick_pipe.predict_proba(df_clean['text'].tolist())\n",
    "predicted = quick_pipe.predict(df_clean['text'].tolist())\n",
    "\n",
    "confidence = np.max(proba, axis=1)\n",
    "disagree = predicted != df_clean['label'].values\n",
    "suspicious = disagree & (confidence > 0.9)\n",
    "\n",
    "n_suspicious = suspicious.sum()\n",
    "print(f\"\\n[Step 3] Found {n_suspicious} suspicious labels (model strongly disagrees)\")\n",
    "print(f\"  These are likely mislabeled. Removing them.\")\n",
    "\n",
    "n_before = len(df_clean)\n",
    "df_clean = df_clean[~suspicious]\n",
    "n_removed = n_before - len(df_clean)\n",
    "cleaning_log.append({'step': 'Remove suspicious labels', 'samples': len(df_clean)})\n",
    "print(f\"  Removed {n_removed} suspicious samples -> {len(df_clean)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Pipeline Summary:\n",
      "==================================================\n",
      "  Original messy data                 790 samples\n",
      "  Remove duplicates                   703 samples  (-87)\n",
      "  Remove short texts                  676 samples  (-27)\n",
      "  Remove suspicious labels            658 samples  (-18)\n",
      "                                       \n",
      "  Total removed                       132 samples (16.7%)\n",
      "\n",
      "Remaining class distribution:\n",
      "  rec.sport.baseball: 594 (90.3%)\n",
      "  sci.space: 64 (9.7%)\n"
     ]
    }
   ],
   "source": [
    "# Show the cleaning pipeline summary\n",
    "print(\"Cleaning Pipeline Summary:\")\n",
    "print(\"=\" * 50)\n",
    "for i, entry in enumerate(cleaning_log):\n",
    "    if i == 0:\n",
    "        print(f\"  {entry['step']:30s}  {entry['samples']:5d} samples\")\n",
    "    else:\n",
    "        removed = cleaning_log[i-1]['samples'] - entry['samples']\n",
    "        print(f\"  {entry['step']:30s}  {entry['samples']:5d} samples  (-{removed})\")\n",
    "\n",
    "total_removed = cleaning_log[0]['samples'] - cleaning_log[-1]['samples']\n",
    "print(f\"{'':30s}  {'':5s}\")\n",
    "print(f\"  {'Total removed':30s}  {total_removed:5d} samples ({total_removed/cleaning_log[0]['samples']*100:.1f}%)\")\n",
    "\n",
    "# Show remaining class distribution\n",
    "print(f\"\\nRemaining class distribution:\")\n",
    "for label in [0, 1]:\n",
    "    count = (df_clean['label'] == label).sum()\n",
    "    print(f\"  {train_data.target_names[label]}: {count} ({count/len(df_clean)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We still have class imbalance after cleaning. We'll handle that at training time\n",
    "using `class_weight='balanced'` in LogisticRegression, which automatically adjusts the loss\n",
    "function to pay more attention to the minority class.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Retrain and Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pristine baseline: 0.966\n",
      "Messy model:       0.752\n",
      "Cleaned model:     0.924\n",
      "\n",
      "Recovered 0.172 accuracy points by cleaning the data.\n"
     ]
    }
   ],
   "source": [
    "# Train on cleaned data WITH class_weight='balanced' to handle remaining imbalance\n",
    "clean_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000, stop_words='english')),\n",
    "    ('clf', LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "clean_pipeline.fit(df_clean['text'].tolist(), df_clean['label'].values)\n",
    "clean_preds = clean_pipeline.predict(test_data.data)\n",
    "clean_accuracy = accuracy_score(test_data.target, clean_preds)\n",
    "\n",
    "print(f\"Pristine baseline: {pristine_accuracy:.3f}\")\n",
    "print(f\"Messy model:       {messy_accuracy:.3f}\")\n",
    "print(f\"Cleaned model:     {clean_accuracy:.3f}\")\n",
    "print(f\"\\nRecovered {clean_accuracy - messy_accuracy:.3f} accuracy points by cleaning the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MESSY MODEL ===\n",
      "              Class Precision Recall    F1  Support\n",
      "rec.sport.baseball     0.640  0.960 0.768      397\n",
      "         sci.space     0.940  0.550 0.694      399\n",
      "           Overall     0.790  0.755 0.731      796\n",
      "\n",
      "=== CLEANED MODEL ===\n",
      "              Class Precision Recall    F1  Support\n",
      "rec.sport.baseball     0.936  0.909 0.922      397\n",
      "         sci.space     0.912  0.939 0.925      399\n",
      "           Overall     0.924  0.924 0.924      796\n",
      "\n",
      "=== PRISTINE BASELINE ===\n",
      "              Class Precision Recall    F1  Support\n",
      "rec.sport.baseball     0.972  0.962 0.967      397\n",
      "         sci.space     0.963  0.972 0.967      399\n",
      "           Overall     0.967  0.967 0.967      796\n"
     ]
    }
   ],
   "source": [
    "# Side-by-side comparison table\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def get_metrics(y_true, y_pred, class_names):\n",
    "    \"\"\"Get per-class and overall metrics.\"\"\"\n",
    "    p, r, f, s = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    rows = []\n",
    "    for i, name in enumerate(class_names):\n",
    "        rows.append({\n",
    "            'Class': name,\n",
    "            'Precision': f'{p[i]:.3f}',\n",
    "            'Recall': f'{r[i]:.3f}',\n",
    "            'F1': f'{f[i]:.3f}',\n",
    "            'Support': int(s[i])\n",
    "        })\n",
    "    rows.append({\n",
    "        'Class': 'Overall',\n",
    "        'Precision': f'{np.mean(p):.3f}',\n",
    "        'Recall': f'{np.mean(r):.3f}',\n",
    "        'F1': f'{np.mean(f):.3f}',\n",
    "        'Support': int(np.sum(s))\n",
    "    })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "print(\"=== MESSY MODEL ===\")\n",
    "messy_metrics = get_metrics(test_data.target, messy_preds, train_data.target_names)\n",
    "print(messy_metrics.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== CLEANED MODEL ===\")\n",
    "clean_metrics = get_metrics(test_data.target, clean_preds, train_data.target_names)\n",
    "print(clean_metrics.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== PRISTINE BASELINE ===\")\n",
    "pristine_metrics = get_metrics(test_data.target, pristine_preds, train_data.target_names)\n",
    "print(pristine_metrics.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality > model complexity. Cleaning the data recovered most of the performance.\n"
     ]
    }
   ],
   "source": [
    "# Side-by-side confusion matrices: messy vs cleaned vs pristine\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, preds, title, cmap in [\n",
    "    (axes[0], messy_preds, f'Messy (acc={messy_accuracy:.3f})', 'Reds'),\n",
    "    (axes[1], clean_preds, f'Cleaned (acc={clean_accuracy:.3f})', 'Blues'),\n",
    "    (axes[2], pristine_preds, f'Pristine (acc={pristine_accuracy:.3f})', 'Greens'),\n",
    "]:\n",
    "    cm = confusion_matrix(test_data.target, preds)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap,\n",
    "                xticklabels=train_data.target_names,\n",
    "                yticklabels=train_data.target_names,\n",
    "                ax=ax)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Data quality > model complexity. Cleaning the data recovered most of the performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaway:** We didn't change the algorithm. We didn't tune hyperparameters. We just\n",
    "cleaned the data. That alone recovered most of the lost performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Build a Data Validation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation functions defined.\n"
     ]
    }
   ],
   "source": [
    "def check_class_balance(labels, max_ratio=3.0):\n",
    "    \"\"\"Check if class distribution is within acceptable bounds.\"\"\"\n",
    "    counts = pd.Series(labels).value_counts()\n",
    "    ratio = counts.max() / counts.min()\n",
    "    passed = ratio <= max_ratio\n",
    "    status = 'PASS' if passed else 'FAIL'\n",
    "    print(f\"  [{status}] Class balance — ratio: {ratio:.1f}:1 (max allowed: {max_ratio:.1f}:1)\")\n",
    "    if not passed:\n",
    "        print(f\"         Distribution: {dict(counts)}\")\n",
    "    return passed\n",
    "\n",
    "def check_duplicates(texts, max_pct=1.0):\n",
    "    \"\"\"Check for exact duplicate texts.\"\"\"\n",
    "    n_dups = pd.Series(texts).duplicated().sum()\n",
    "    pct = n_dups / len(texts) * 100\n",
    "    passed = pct <= max_pct\n",
    "    status = 'PASS' if passed else 'FAIL'\n",
    "    print(f\"  [{status}] Duplicates — {n_dups} found ({pct:.1f}%, max allowed: {max_pct:.1f}%)\")\n",
    "    return passed\n",
    "\n",
    "def check_empty_texts(texts, max_pct=0.5):\n",
    "    \"\"\"Check for empty or very short texts (< 10 chars).\"\"\"\n",
    "    lengths = pd.Series(texts).str.len()\n",
    "    n_short = (lengths < 10).sum()\n",
    "    pct = n_short / len(texts) * 100\n",
    "    passed = pct <= max_pct\n",
    "    status = 'PASS' if passed else 'FAIL'\n",
    "    print(f\"  [{status}] Short/empty texts — {n_short} found ({pct:.1f}%, max allowed: {max_pct:.1f}%)\")\n",
    "    return passed\n",
    "\n",
    "def check_text_length_distribution(texts, min_median=100, max_std_ratio=5.0):\n",
    "    \"\"\"Check that text lengths are reasonable.\"\"\"\n",
    "    lengths = pd.Series(texts).str.len()\n",
    "    median_len = lengths.median()\n",
    "    std_ratio = lengths.std() / lengths.median() if lengths.median() > 0 else float('inf')\n",
    "    passed = median_len >= min_median and std_ratio <= max_std_ratio\n",
    "    status = 'PASS' if passed else 'FAIL'\n",
    "    print(f\"  [{status}] Text length — median: {median_len:.0f} chars, std/median ratio: {std_ratio:.2f}\")\n",
    "    if not passed:\n",
    "        print(f\"         Expected median >= {min_median}, std/median <= {max_std_ratio}\")\n",
    "    return passed\n",
    "\n",
    "print(\"Validation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation on the MESSY data:\n",
      "\n",
      "============================================================\n",
      "  DATA VALIDATION: Messy Training Data\n",
      "  Samples: 790\n",
      "============================================================\n",
      "  [FAIL] Class balance — ratio: 9.1:1 (max allowed: 3.0:1)\n",
      "         Distribution: {0: 712, 1: 78}\n",
      "  [FAIL] Duplicates — 87 found (11.0%, max allowed: 1.0%)\n",
      "  [FAIL] Short/empty texts — 27 found (3.4%, max allowed: 0.5%)\n",
      "  [PASS] Text length — median: 1327 chars, std/median ratio: 1.95\n",
      "\n",
      "  VALIDATION FAILED — 1/4 checks passed\n",
      "============================================================\n",
      "\n",
      "Running validation on the CLEANED data:\n",
      "\n",
      "============================================================\n",
      "  DATA VALIDATION: Cleaned Training Data\n",
      "  Samples: 658\n",
      "============================================================\n",
      "  [FAIL] Class balance — ratio: 9.3:1 (max allowed: 3.0:1)\n",
      "         Distribution: {0: 594, 1: 64}\n",
      "  [PASS] Duplicates — 0 found (0.0%, max allowed: 1.0%)\n",
      "  [PASS] Short/empty texts — 0 found (0.0%, max allowed: 0.5%)\n",
      "  [PASS] Text length — median: 1382 chars, std/median ratio: 1.82\n",
      "\n",
      "  VALIDATION FAILED — 3/4 checks passed\n",
      "============================================================\n",
      "\n",
      "Running validation on the PRISTINE data:\n",
      "\n",
      "============================================================\n",
      "  DATA VALIDATION: Pristine Training Data\n",
      "  Samples: 1197\n",
      "============================================================\n",
      "  [PASS] Class balance — ratio: 1.0:1 (max allowed: 3.0:1)\n",
      "  [PASS] Duplicates — 0 found (0.0%, max allowed: 1.0%)\n",
      "  [PASS] Short/empty texts — 0 found (0.0%, max allowed: 0.5%)\n",
      "  [PASS] Text length — median: 1467 chars, std/median ratio: 1.78\n",
      "\n",
      "  ALL CHECKS PASSED — 4/4 checks passed\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def validate_dataset(texts, labels, dataset_name=\"dataset\"):\n",
    "    \"\"\"Run all validation checks on a dataset. Returns True if all pass.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  DATA VALIDATION: {dataset_name}\")\n",
    "    print(f\"  Samples: {len(texts)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    results = []\n",
    "    results.append(check_class_balance(labels))\n",
    "    results.append(check_duplicates(texts))\n",
    "    results.append(check_empty_texts(texts))\n",
    "    results.append(check_text_length_distribution(texts))\n",
    "    \n",
    "    all_passed = all(results)\n",
    "    print(f\"\\n  {'ALL CHECKS PASSED' if all_passed else 'VALIDATION FAILED'} — \"\n",
    "          f\"{sum(results)}/{len(results)} checks passed\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "# Run on the messy data (should FAIL)\n",
    "print(\"Running validation on the MESSY data:\")\n",
    "messy_passed = validate_dataset(messy_texts, messy_labels, \"Messy Training Data\")\n",
    "\n",
    "# Run on the cleaned data (should PASS)\n",
    "print(\"\\nRunning validation on the CLEANED data:\")\n",
    "clean_passed = validate_dataset(\n",
    "    df_clean['text'].tolist(),\n",
    "    df_clean['label'].values.tolist(),\n",
    "    \"Cleaned Training Data\"\n",
    ")\n",
    "\n",
    "# Run on the original pristine data (should PASS)\n",
    "print(\"\\nRunning validation on the PRISTINE data:\")\n",
    "pristine_passed = validate_dataset(\n",
    "    train_data.data,\n",
    "    train_data.target.tolist(),\n",
    "    \"Pristine Training Data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Pre-training gate\n",
      "\n",
      "Attempt 1: Train on messy data\n",
      "\n",
      "============================================================\n",
      "  DATA VALIDATION: Pre-Training Check\n",
      "  Samples: 790\n",
      "============================================================\n",
      "  [FAIL] Class balance — ratio: 9.1:1 (max allowed: 3.0:1)\n",
      "         Distribution: {0: 712, 1: 78}\n",
      "  [FAIL] Duplicates — 87 found (11.0%, max allowed: 1.0%)\n",
      "  [FAIL] Short/empty texts — 27 found (3.4%, max allowed: 0.5%)\n",
      "  [PASS] Text length — median: 1327 chars, std/median ratio: 1.95\n",
      "\n",
      "  VALIDATION FAILED — 1/4 checks passed\n",
      "============================================================\n",
      "\n",
      "  TRAINING BLOCKED — fix data quality issues first.\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Attempt 2: Train on cleaned data\n",
      "\n",
      "============================================================\n",
      "  DATA VALIDATION: Pre-Training Check\n",
      "  Samples: 658\n",
      "============================================================\n",
      "  [FAIL] Class balance — ratio: 9.3:1 (max allowed: 3.0:1)\n",
      "         Distribution: {0: 594, 1: 64}\n",
      "  [PASS] Duplicates — 0 found (0.0%, max allowed: 1.0%)\n",
      "  [PASS] Short/empty texts — 0 found (0.0%, max allowed: 0.5%)\n",
      "  [PASS] Text length — median: 1382 chars, std/median ratio: 1.82\n",
      "\n",
      "  VALIDATION FAILED — 3/4 checks passed\n",
      "============================================================\n",
      "\n",
      "  TRAINING BLOCKED — fix data quality issues first.\n"
     ]
    }
   ],
   "source": [
    "# Show how this would work as a pre-training gate\n",
    "print(\"Example: Pre-training gate\\n\")\n",
    "\n",
    "def train_with_validation(texts, labels, test_texts, test_labels, class_names):\n",
    "    \"\"\"Only train if data passes validation.\"\"\"\n",
    "    passed = validate_dataset(texts, labels, \"Pre-Training Check\")\n",
    "    \n",
    "    if not passed:\n",
    "        print(\"\\n  TRAINING BLOCKED — fix data quality issues first.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\n  Validation passed. Training model...\")\n",
    "    pipe = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=10000, stop_words='english')),\n",
    "        ('clf', LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42))\n",
    "    ])\n",
    "    pipe.fit(texts, labels)\n",
    "    preds = pipe.predict(test_texts)\n",
    "    acc = accuracy_score(test_labels, preds)\n",
    "    print(f\"  Model trained. Test accuracy: {acc:.3f}\")\n",
    "    return pipe\n",
    "\n",
    "# Try with messy data — should be blocked\n",
    "print(\"Attempt 1: Train on messy data\")\n",
    "result = train_with_validation(messy_texts, messy_labels,\n",
    "                                test_data.data, test_data.target,\n",
    "                                train_data.target_names)\n",
    "\n",
    "# Try with clean data — should proceed\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"\\nAttempt 2: Train on cleaned data\")\n",
    "result = train_with_validation(df_clean['text'].tolist(),\n",
    "                                df_clean['label'].values.tolist(),\n",
    "                                test_data.data, test_data.target,\n",
    "                                train_data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**These checks should run before every training job.** In a real ML pipeline, you'd wire\n",
    "these into your CI/CD or orchestration system so that bad data never silently reaches the model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You just experienced the most common reason ML models fail in production: bad data.\n",
    "\n",
    "| Concept | What You Learned |\n",
    "|---------|------------------|\n",
    "| **Data corruption** | Duplicates, mislabels, garbage, and imbalance silently destroy model performance |\n",
    "| **EDA** | Always explore your data before training — plots and stats catch what code doesn't |\n",
    "| **Cleaning pipeline** | Step-by-step deduplication, filtering, and label validation |\n",
    "| **class_weight='balanced'** | A simple way to handle class imbalance at training time |\n",
    "| **Validation gates** | Automated checks that block training when data quality is too low |\n",
    "| **Data quality > model complexity** | Cleaning data improves results more than a fancier algorithm |\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **ML Lab 03**, you'll take a trained model and deploy it as a live API — complete with\n",
    "health checks, Prometheus metrics, and a Grafana dashboard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
