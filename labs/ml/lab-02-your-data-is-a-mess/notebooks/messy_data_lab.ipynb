{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Lab 02: Your Data Is a Mess\n",
    "\n",
    "In Lab 1 you trained a model on clean, balanced data and got ~97% accuracy. That was the\n",
    "parking lot. Real data is the highway.\n",
    "\n",
    "In this lab, you'll intentionally corrupt a dataset to simulate the kinds of problems you'll\n",
    "hit in production: duplicates, mislabeled examples, garbage strings, and class imbalance.\n",
    "You'll watch your model fall apart, learn to diagnose the issues with EDA, clean the data,\n",
    "and build validation checks that catch these problems before they reach your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: The Pristine Baseline\n",
    "\n",
    "First, let's establish a clean baseline. Same 20 Newsgroups dataset, same pipeline from Lab 1.\n",
    "This is the score you're trying to recover after we trash the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the same dataset from Lab 1\n",
    "categories = ['rec.sport.baseball', 'sci.space']\n",
    "\n",
    "train_data = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "test_data = fetch_20newsgroups(\n",
    "    subset='test',\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_data.data)}\")\n",
    "print(f\"Test samples:     {len(test_data.data)}\")\n",
    "print(f\"Classes:          {train_data.target_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the pristine baseline model\n",
    "pristine_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000, stop_words='english')),\n",
    "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "pristine_pipeline.fit(train_data.data, train_data.target)\n",
    "pristine_preds = pristine_pipeline.predict(test_data.data)\n",
    "pristine_accuracy = accuracy_score(test_data.target, pristine_preds)\n",
    "\n",
    "print(f\"Pristine baseline accuracy: {pristine_accuracy:.3f}\")\n",
    "print(f\"\\nThis is the parking lot. Real data is the highway.\")\n",
    "print(f\"\\nFull classification report:\")\n",
    "print(classification_report(test_data.target, pristine_preds,\n",
    "                            target_names=train_data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bookmark this number.** We'll see how far it drops when the data gets messy, and how close we can recover.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Meet Your Messy Data\n",
    "\n",
    "Real-world data is never this clean. Let's simulate the four most common data quality problems:\n",
    "\n",
    "1. **Duplicates** (~10% of samples repeated) — inflates the dataset, biases the model toward duplicated examples\n",
    "2. **Mislabeled examples** (~5% of labels flipped) — teaches the model the wrong patterns\n",
    "3. **Garbage strings** (~3% empty or random characters) — adds noise that confuses the vectorizer\n",
    "4. **Class imbalance** (90/10 split) — the model learns to always predict the majority class\n",
    "\n",
    "We'll apply each corruption one at a time so you can see the individual damage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Start with a clean copy\n",
    "messy_texts = list(train_data.data)\n",
    "messy_labels = list(train_data.target)\n",
    "\n",
    "print(f\"Starting with {len(messy_texts)} clean samples\")\n",
    "print(f\"Class distribution: {sum(np.array(messy_labels) == 0)} baseball, {sum(np.array(messy_labels) == 1)} space\")\n",
    "\n",
    "# --- Corruption 1: Inject duplicates (~10%) ---\n",
    "n_dups = int(len(messy_texts) * 0.10)\n",
    "dup_idx = np.random.choice(len(messy_texts), n_dups, replace=False)\n",
    "for i in dup_idx:\n",
    "    messy_texts.append(messy_texts[i])\n",
    "    messy_labels.append(messy_labels[i])\n",
    "print(f\"\\n[1] Injected {n_dups} duplicates -> {len(messy_texts)} samples\")\n",
    "\n",
    "# --- Corruption 2: Flip labels (~5%) ---\n",
    "n_flip = int(len(messy_texts) * 0.05)\n",
    "flip_idx = np.random.choice(len(messy_texts), n_flip, replace=False)\n",
    "for i in flip_idx:\n",
    "    messy_labels[i] = 1 - messy_labels[i]\n",
    "print(f\"[2] Flipped {n_flip} labels\")\n",
    "\n",
    "# --- Corruption 3: Inject garbage strings (~3%) ---\n",
    "garbage_strings = [\n",
    "    \"\",\n",
    "    \" \",\n",
    "    \"asdf\",\n",
    "    \"xxx\",\n",
    "    \"!!!\",\n",
    "    \"null\",\n",
    "    \"N/A\",\n",
    "    \"test\",\n",
    "    \".\",\n",
    "    \"---\",\n",
    "]\n",
    "n_garbage = int(len(messy_texts) * 0.03)\n",
    "garbage_idx = np.random.choice(len(messy_texts), n_garbage, replace=False)\n",
    "for i in garbage_idx:\n",
    "    messy_texts[i] = np.random.choice(garbage_strings)\n",
    "print(f\"[3] Replaced {n_garbage} samples with garbage strings\")\n",
    "\n",
    "# --- Corruption 4: Create class imbalance (90/10) ---\n",
    "# Keep all baseball (class 0), subsample space (class 1) to 10% of total\n",
    "messy_arr = np.array(messy_labels)\n",
    "class0_idx = np.where(messy_arr == 0)[0]\n",
    "class1_idx = np.where(messy_arr == 1)[0]\n",
    "\n",
    "# Target: class 1 should be ~10% of the final dataset\n",
    "target_class1 = int(len(class0_idx) * 0.11)  # ~10% of total\n",
    "if len(class1_idx) > target_class1:\n",
    "    keep_class1 = np.random.choice(class1_idx, target_class1, replace=False)\n",
    "    keep_idx = np.concatenate([class0_idx, keep_class1])\n",
    "    np.random.shuffle(keep_idx)\n",
    "    messy_texts = [messy_texts[i] for i in keep_idx]\n",
    "    messy_labels = [messy_labels[i] for i in keep_idx]\n",
    "\n",
    "messy_arr = np.array(messy_labels)\n",
    "print(f\"[4] Created class imbalance -> {len(messy_texts)} samples\")\n",
    "print(f\"    Class 0 (baseball): {sum(messy_arr == 0)} ({sum(messy_arr == 0)/len(messy_arr)*100:.1f}%)\")\n",
    "print(f\"    Class 1 (space):    {sum(messy_arr == 1)} ({sum(messy_arr == 1)/len(messy_arr)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SAME model on the messy data\n",
    "messy_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000, stop_words='english')),\n",
    "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "messy_pipeline.fit(messy_texts, messy_labels)\n",
    "messy_preds = messy_pipeline.predict(test_data.data)\n",
    "messy_accuracy = accuracy_score(test_data.target, messy_preds)\n",
    "\n",
    "print(f\"Pristine accuracy: {pristine_accuracy:.3f}\")\n",
    "print(f\"Messy accuracy:    {messy_accuracy:.3f}\")\n",
    "print(f\"Drop:              {pristine_accuracy - messy_accuracy:.3f}\")\n",
    "print(f\"\\nFull classification report on messy model:\")\n",
    "print(classification_report(test_data.target, messy_preds,\n",
    "                            target_names=train_data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side confusion matrices: pristine vs messy\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "cm_pristine = confusion_matrix(test_data.target, pristine_preds)\n",
    "sns.heatmap(cm_pristine, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=train_data.target_names,\n",
    "            yticklabels=train_data.target_names,\n",
    "            ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title(f'Pristine Model (acc={pristine_accuracy:.3f})')\n",
    "\n",
    "cm_messy = confusion_matrix(test_data.target, messy_preds)\n",
    "sns.heatmap(cm_messy, annot=True, fmt='d', cmap='Reds',\n",
    "            xticklabels=train_data.target_names,\n",
    "            yticklabels=train_data.target_names,\n",
    "            ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title(f'Messy Model (acc={messy_accuracy:.3f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Same algorithm, same test set, different data quality. Data quality > model complexity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The model didn't get dumber — the data got worse.** Same algorithm, same hyperparameters.\n",
    "The only thing that changed was the quality of the training data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Explore Before You Train (EDA)\n",
    "\n",
    "Before you try to fix anything, you need to *understand* what's wrong. This is **Exploratory Data\n",
    "Analysis** (EDA) — the step most people skip and then regret.\n",
    "\n",
    "We'll check:\n",
    "1. Class distribution\n",
    "2. Text length distribution\n",
    "3. Duplicates (exact and near-duplicates)\n",
    "4. Empty/very short texts\n",
    "5. Random samples to spot-check labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put messy data into a DataFrame for easier analysis\n",
    "df = pd.DataFrame({\n",
    "    'text': messy_texts,\n",
    "    'label': messy_labels\n",
    "})\n",
    "df['text_length'] = df['text'].str.len()\n",
    "df['word_count'] = df['text'].str.split().str.len().fillna(0).astype(int)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nBasic stats:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 1: Class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "class_counts = df['label'].value_counts().sort_index()\n",
    "class_names = [train_data.target_names[i] for i in class_counts.index]\n",
    "colors = ['#2196F3', '#FF9800']\n",
    "\n",
    "axes[0].bar(class_names, class_counts.values, color=colors)\n",
    "axes[0].set_title('Class Distribution (Messy Data)')\n",
    "axes[0].set_ylabel('Count')\n",
    "for i, (name, count) in enumerate(zip(class_names, class_counts.values)):\n",
    "    axes[0].text(i, count + 5, f'{count}\\n({count/len(df)*100:.1f}%)',\n",
    "                ha='center', fontweight='bold')\n",
    "\n",
    "# Compare with what clean data looks like\n",
    "clean_counts = pd.Series(train_data.target).value_counts().sort_index()\n",
    "clean_names = [train_data.target_names[i] for i in clean_counts.index]\n",
    "axes[1].bar(clean_names, clean_counts.values, color=colors, alpha=0.7)\n",
    "axes[1].set_title('Class Distribution (Clean Data)')\n",
    "axes[1].set_ylabel('Count')\n",
    "for i, (name, count) in enumerate(zip(clean_names, clean_counts.values)):\n",
    "    axes[1].text(i, count + 5, f'{count}\\n({count/len(train_data.target)*100:.1f}%)',\n",
    "                ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "ratio = class_counts.max() / class_counts.min()\n",
    "print(f\"Imbalance ratio: {ratio:.1f}:1\")\n",
    "print(f\"A ratio above 3:1 is a red flag. Above 10:1 is a problem.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 2: Text length distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram of text lengths\n",
    "axes[0].hist(df['text_length'], bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Text Length Distribution')\n",
    "axes[0].set_xlabel('Character count')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].axvline(x=10, color='red', linestyle='--', label='Min threshold (10 chars)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Zoom in on the short texts\n",
    "short_texts = df[df['text_length'] < 50]\n",
    "axes[1].hist(short_texts['text_length'], bins=20, color='tomato', edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title(f'Short Texts (< 50 chars) — {len(short_texts)} samples')\n",
    "axes[1].set_xlabel('Character count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(x=10, color='red', linestyle='--', label='Min threshold (10 chars)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Texts shorter than 10 chars: {len(df[df['text_length'] < 10])}\")\n",
    "print(f\"Empty texts: {len(df[df['text_length'] == 0])}\")\n",
    "print(f\"\\nSample short texts:\")\n",
    "for _, row in df[df['text_length'] < 10].head(10).iterrows():\n",
    "    print(f\"  label={row['label']} len={row['text_length']}: '{row['text']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 3: Duplicates\n",
    "n_exact_dups = df.duplicated(subset='text').sum()\n",
    "print(f\"Exact duplicates: {n_exact_dups} ({n_exact_dups/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Show some duplicated texts\n",
    "dup_mask = df.duplicated(subset='text', keep=False)\n",
    "dup_df = df[dup_mask].sort_values('text')\n",
    "if len(dup_df) > 0:\n",
    "    print(f\"\\nSample duplicated entries (showing first 5 groups):\")\n",
    "    seen = set()\n",
    "    count = 0\n",
    "    for _, row in dup_df.iterrows():\n",
    "        text_preview = row['text'][:80].replace('\\n', ' ')\n",
    "        if text_preview not in seen:\n",
    "            seen.add(text_preview)\n",
    "            n_copies = len(df[df['text'] == row['text']])\n",
    "            print(f\"  [{n_copies} copies] label={row['label']}: \\\"{text_preview}...\\\"\")\n",
    "            count += 1\n",
    "            if count >= 5:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 4: Spot-check labels by sampling random examples\n",
    "print(\"Random samples from each class — do the labels look right?\\n\")\n",
    "\n",
    "for label in [0, 1]:\n",
    "    class_name = train_data.target_names[label]\n",
    "    class_df = df[(df['label'] == label) & (df['text_length'] > 50)]\n",
    "    samples = class_df.sample(n=min(3, len(class_df)), random_state=42)\n",
    "    \n",
    "    print(f\"=== {class_name} (label={label}) ===\")\n",
    "    for _, row in samples.iterrows():\n",
    "        preview = row['text'][:150].replace('\\n', ' ')\n",
    "        print(f\"  \\\"{preview}...\\\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EDA Summary:**\n",
    "- Heavy class imbalance (look at the bar chart)\n",
    "- Duplicate entries inflating the dataset\n",
    "- Garbage/empty strings that the model will try to learn from\n",
    "- Some labels might be wrong (hard to tell without domain knowledge — we'll use a trick in the next section)\n",
    "\n",
    "**Rule:** Never train on data you haven't explored. 10 minutes of EDA saves hours of debugging model performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Clean It Up\n",
    "\n",
    "Now let's fix each issue step by step. We'll track the sample count at each step\n",
    "so you can see exactly what we're removing and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the messy DataFrame\n",
    "df_clean = df.copy()\n",
    "cleaning_log = [{'step': 'Original messy data', 'samples': len(df_clean)}]\n",
    "print(f\"Starting: {len(df_clean)} samples\")\n",
    "\n",
    "# --- Step 1: Remove exact duplicates ---\n",
    "n_before = len(df_clean)\n",
    "df_clean = df_clean.drop_duplicates(subset='text', keep='first')\n",
    "n_removed = n_before - len(df_clean)\n",
    "cleaning_log.append({'step': 'Remove duplicates', 'samples': len(df_clean)})\n",
    "print(f\"\\n[Step 1] Removed {n_removed} exact duplicates -> {len(df_clean)} samples\")\n",
    "\n",
    "# --- Step 2: Remove empty/very short texts ---\n",
    "n_before = len(df_clean)\n",
    "df_clean = df_clean[df_clean['text_length'] >= 10]\n",
    "n_removed = n_before - len(df_clean)\n",
    "cleaning_log.append({'step': 'Remove short texts', 'samples': len(df_clean)})\n",
    "print(f\"[Step 2] Removed {n_removed} short/empty texts -> {len(df_clean)} samples\")\n",
    "\n",
    "# --- Step 3: Flag suspicious labels ---\n",
    "# Train a quick model on the current data to find samples where the model's\n",
    "# prediction strongly disagrees with the label (potential mislabels)\n",
    "quick_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000, stop_words='english')),\n",
    "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "quick_pipe.fit(df_clean['text'].tolist(), df_clean['label'].values)\n",
    "proba = quick_pipe.predict_proba(df_clean['text'].tolist())\n",
    "predicted = quick_pipe.predict(df_clean['text'].tolist())\n",
    "\n",
    "# Flag samples where: prediction disagrees with label AND confidence is high\n",
    "confidence = np.max(proba, axis=1)\n",
    "disagree = predicted != df_clean['label'].values\n",
    "suspicious = disagree & (confidence > 0.9)\n",
    "\n",
    "n_suspicious = suspicious.sum()\n",
    "print(f\"\\n[Step 3] Found {n_suspicious} suspicious labels (model strongly disagrees)\")\n",
    "print(f\"  These are likely mislabeled. Removing them.\")\n",
    "\n",
    "n_before = len(df_clean)\n",
    "df_clean = df_clean[~suspicious]\n",
    "n_removed = n_before - len(df_clean)\n",
    "cleaning_log.append({'step': 'Remove suspicious labels', 'samples': len(df_clean)})\n",
    "print(f\"  Removed {n_removed} suspicious samples -> {len(df_clean)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the cleaning pipeline summary\n",
    "print(\"Cleaning Pipeline Summary:\")\n",
    "print(\"=\" * 50)\n",
    "for i, entry in enumerate(cleaning_log):\n",
    "    if i == 0:\n",
    "        print(f\"  {entry['step']:30s}  {entry['samples']:5d} samples\")\n",
    "    else:\n",
    "        removed = cleaning_log[i-1]['samples'] - entry['samples']\n",
    "        print(f\"  {entry['step']:30s}  {entry['samples']:5d} samples  (-{removed})\")\n",
    "\n",
    "total_removed = cleaning_log[0]['samples'] - cleaning_log[-1]['samples']\n",
    "print(f\"{'':30s}  {'':5s}\")\n",
    "print(f\"  {'Total removed':30s}  {total_removed:5d} samples ({total_removed/cleaning_log[0]['samples']*100:.1f}%)\")\n",
    "\n",
    "# Show remaining class distribution\n",
    "print(f\"\\nRemaining class distribution:\")\n",
    "for label in [0, 1]:\n",
    "    count = (df_clean['label'] == label).sum()\n",
    "    print(f\"  {train_data.target_names[label]}: {count} ({count/len(df_clean)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We still have class imbalance after cleaning. We'll handle that at training time\n",
    "using `class_weight='balanced'` in LogisticRegression, which automatically adjusts the loss\n",
    "function to pay more attention to the minority class.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Retrain and Compare\n",
    "\n",
    "Now let's train on the cleaned data and see how much we recovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on cleaned data WITH class_weight='balanced' to handle remaining imbalance\n",
    "clean_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000, stop_words='english')),\n",
    "    ('clf', LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "clean_pipeline.fit(df_clean['text'].tolist(), df_clean['label'].values)\n",
    "clean_preds = clean_pipeline.predict(test_data.data)\n",
    "clean_accuracy = accuracy_score(test_data.target, clean_preds)\n",
    "\n",
    "print(f\"Pristine baseline: {pristine_accuracy:.3f}\")\n",
    "print(f\"Messy model:       {messy_accuracy:.3f}\")\n",
    "print(f\"Cleaned model:     {clean_accuracy:.3f}\")\n",
    "print(f\"\\nRecovered {clean_accuracy - messy_accuracy:.3f} accuracy points by cleaning the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison table\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def get_metrics(y_true, y_pred, class_names):\n",
    "    \"\"\"Get per-class and overall metrics.\"\"\"\n",
    "    p, r, f, s = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    rows = []\n",
    "    for i, name in enumerate(class_names):\n",
    "        rows.append({\n",
    "            'Class': name,\n",
    "            'Precision': f'{p[i]:.3f}',\n",
    "            'Recall': f'{r[i]:.3f}',\n",
    "            'F1': f'{f[i]:.3f}',\n",
    "            'Support': int(s[i])\n",
    "        })\n",
    "    rows.append({\n",
    "        'Class': 'Overall',\n",
    "        'Precision': f'{np.mean(p):.3f}',\n",
    "        'Recall': f'{np.mean(r):.3f}',\n",
    "        'F1': f'{np.mean(f):.3f}',\n",
    "        'Support': int(np.sum(s))\n",
    "    })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "print(\"=== MESSY MODEL ===\")\n",
    "messy_metrics = get_metrics(test_data.target, messy_preds, train_data.target_names)\n",
    "print(messy_metrics.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== CLEANED MODEL ===\")\n",
    "clean_metrics = get_metrics(test_data.target, clean_preds, train_data.target_names)\n",
    "print(clean_metrics.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== PRISTINE BASELINE ===\")\n",
    "pristine_metrics = get_metrics(test_data.target, pristine_preds, train_data.target_names)\n",
    "print(pristine_metrics.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side confusion matrices: messy vs cleaned vs pristine\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, preds, title, cmap in [\n",
    "    (axes[0], messy_preds, f'Messy (acc={messy_accuracy:.3f})', 'Reds'),\n",
    "    (axes[1], clean_preds, f'Cleaned (acc={clean_accuracy:.3f})', 'Blues'),\n",
    "    (axes[2], pristine_preds, f'Pristine (acc={pristine_accuracy:.3f})', 'Greens'),\n",
    "]:\n",
    "    cm = confusion_matrix(test_data.target, preds)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap,\n",
    "                xticklabels=train_data.target_names,\n",
    "                yticklabels=train_data.target_names,\n",
    "                ax=ax)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Data quality > model complexity. Cleaning the data recovered most of the performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaway:** We didn't change the algorithm. We didn't tune hyperparameters. We just\n",
    "cleaned the data. That alone recovered most of the lost performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Build a Data Validation Pipeline\n",
    "\n",
    "The cleaning we just did was manual. In production, you need **automated checks** that run\n",
    "before every training job. If the data fails validation, training should not proceed.\n",
    "\n",
    "Let's build simple validation functions that would have caught every issue from Section 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_class_balance(labels, max_ratio=3.0):\n",
    "    \"\"\"Check if class distribution is within acceptable bounds.\"\"\"\n",
    "    counts = pd.Series(labels).value_counts()\n",
    "    ratio = counts.max() / counts.min()\n",
    "    passed = ratio <= max_ratio\n",
    "    status = 'PASS' if passed else 'FAIL'\n",
    "    print(f\"  [{status}] Class balance — ratio: {ratio:.1f}:1 (max allowed: {max_ratio:.1f}:1)\")\n",
    "    if not passed:\n",
    "        print(f\"         Distribution: {dict(counts)}\")\n",
    "    return passed\n",
    "\n",
    "def check_duplicates(texts, max_pct=1.0):\n",
    "    \"\"\"Check for exact duplicate texts.\"\"\"\n",
    "    n_dups = pd.Series(texts).duplicated().sum()\n",
    "    pct = n_dups / len(texts) * 100\n",
    "    passed = pct <= max_pct\n",
    "    status = 'PASS' if passed else 'FAIL'\n",
    "    print(f\"  [{status}] Duplicates — {n_dups} found ({pct:.1f}%, max allowed: {max_pct:.1f}%)\")\n",
    "    return passed\n",
    "\n",
    "def check_empty_texts(texts, max_pct=0.5):\n",
    "    \"\"\"Check for empty or very short texts (< 10 chars).\"\"\"\n",
    "    lengths = pd.Series(texts).str.len()\n",
    "    n_short = (lengths < 10).sum()\n",
    "    pct = n_short / len(texts) * 100\n",
    "    passed = pct <= max_pct\n",
    "    status = 'PASS' if passed else 'FAIL'\n",
    "    print(f\"  [{status}] Short/empty texts — {n_short} found ({pct:.1f}%, max allowed: {max_pct:.1f}%)\")\n",
    "    return passed\n",
    "\n",
    "def check_text_length_distribution(texts, min_median=100, max_std_ratio=5.0):\n",
    "    \"\"\"Check that text lengths are reasonable.\"\"\"\n",
    "    lengths = pd.Series(texts).str.len()\n",
    "    median_len = lengths.median()\n",
    "    std_ratio = lengths.std() / lengths.median() if lengths.median() > 0 else float('inf')\n",
    "    passed = median_len >= min_median and std_ratio <= max_std_ratio\n",
    "    status = 'PASS' if passed else 'FAIL'\n",
    "    print(f\"  [{status}] Text length — median: {median_len:.0f} chars, std/median ratio: {std_ratio:.2f}\")\n",
    "    if not passed:\n",
    "        print(f\"         Expected median >= {min_median}, std/median <= {max_std_ratio}\")\n",
    "    return passed\n",
    "\n",
    "print(\"Validation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataset(texts, labels, dataset_name=\"dataset\"):\n",
    "    \"\"\"Run all validation checks on a dataset. Returns True if all pass.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  DATA VALIDATION: {dataset_name}\")\n",
    "    print(f\"  Samples: {len(texts)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    results = []\n",
    "    results.append(check_class_balance(labels))\n",
    "    results.append(check_duplicates(texts))\n",
    "    results.append(check_empty_texts(texts))\n",
    "    results.append(check_text_length_distribution(texts))\n",
    "    \n",
    "    all_passed = all(results)\n",
    "    print(f\"\\n  {'ALL CHECKS PASSED' if all_passed else 'VALIDATION FAILED'} — \"\n",
    "          f\"{sum(results)}/{len(results)} checks passed\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "# Run on the messy data (should FAIL)\n",
    "print(\"Running validation on the MESSY data:\")\n",
    "messy_passed = validate_dataset(messy_texts, messy_labels, \"Messy Training Data\")\n",
    "\n",
    "# Run on the cleaned data (should PASS)\n",
    "print(\"\\nRunning validation on the CLEANED data:\")\n",
    "clean_passed = validate_dataset(\n",
    "    df_clean['text'].tolist(),\n",
    "    df_clean['label'].values.tolist(),\n",
    "    \"Cleaned Training Data\"\n",
    ")\n",
    "\n",
    "# Run on the original pristine data (should PASS)\n",
    "print(\"\\nRunning validation on the PRISTINE data:\")\n",
    "pristine_passed = validate_dataset(\n",
    "    train_data.data,\n",
    "    train_data.target.tolist(),\n",
    "    \"Pristine Training Data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how this would work as a pre-training gate\n",
    "print(\"Example: Pre-training gate\\n\")\n",
    "\n",
    "def train_with_validation(texts, labels, test_texts, test_labels, class_names):\n",
    "    \"\"\"Only train if data passes validation.\"\"\"\n",
    "    passed = validate_dataset(texts, labels, \"Pre-Training Check\")\n",
    "    \n",
    "    if not passed:\n",
    "        print(\"\\n  TRAINING BLOCKED — fix data quality issues first.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\n  Validation passed. Training model...\")\n",
    "    pipe = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=10000, stop_words='english')),\n",
    "        ('clf', LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42))\n",
    "    ])\n",
    "    pipe.fit(texts, labels)\n",
    "    preds = pipe.predict(test_texts)\n",
    "    acc = accuracy_score(test_labels, preds)\n",
    "    print(f\"  Model trained. Test accuracy: {acc:.3f}\")\n",
    "    return pipe\n",
    "\n",
    "# Try with messy data — should be blocked\n",
    "print(\"Attempt 1: Train on messy data\")\n",
    "result = train_with_validation(messy_texts, messy_labels,\n",
    "                                test_data.data, test_data.target,\n",
    "                                train_data.target_names)\n",
    "\n",
    "# Try with clean data — should proceed\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"\\nAttempt 2: Train on cleaned data\")\n",
    "result = train_with_validation(df_clean['text'].tolist(),\n",
    "                                df_clean['label'].values.tolist(),\n",
    "                                test_data.data, test_data.target,\n",
    "                                train_data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**These checks should run before every training job.** In a real ML pipeline, you'd wire\n",
    "these into your CI/CD or orchestration system so that bad data never silently reaches the model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You just experienced the most common reason ML models fail in production: bad data.\n",
    "\n",
    "| Concept | What You Learned |\n",
    "|---------|------------------|\n",
    "| **Data corruption** | Duplicates, mislabels, garbage, and imbalance silently destroy model performance |\n",
    "| **EDA** | Always explore your data before training — plots and stats catch what code doesn't |\n",
    "| **Cleaning pipeline** | Step-by-step deduplication, filtering, and label validation |\n",
    "| **class_weight='balanced'** | A simple way to handle class imbalance at training time |\n",
    "| **Validation gates** | Automated checks that block training when data quality is too low |\n",
    "| **Data quality > model complexity** | Cleaning data improves results more than a fancier algorithm |\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **ML Lab 03**, you'll take a trained model and deploy it as a live API — complete with\n",
    "health checks, Prometheus metrics, and a Grafana dashboard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
