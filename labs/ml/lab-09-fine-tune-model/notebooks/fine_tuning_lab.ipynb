{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# ML Lab 09: Fine-Tune a Model\n",
    "\n",
    "Pre-trained models like BERT have already read the internet. They understand language — grammar,\n",
    "meaning, context. But they don't know *your* task. In this lab, you'll take DistilBERT (a smaller,\n",
    "faster version of BERT), see what it already knows, then fine-tune it to classify baseball vs space\n",
    "posts. A few hundred examples and three epochs later, you'll have a specialist model that crushes\n",
    "zero-shot performance.\n",
    "\n",
    "This is how real ML teams work: start with a pre-trained foundation model, adapt it with your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: What Does a Pre-trained Model Already Know?\n",
    "\n",
    "DistilBERT was trained on Wikipedia and BookCorpus — millions of documents. It learned to predict\n",
    "missing words, and in doing so, it learned the *structure* of language.\n",
    "\n",
    "Let's load it and see: does it already know that baseball sentences are similar to each other,\n",
    "and different from space sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-distilbert",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained DistilBERT model and tokenizer\n",
    "print(\"Loading DistilBERT (this downloads ~260 MB the first time)...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "base_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "base_model.eval()\n",
    "print(f\"Model loaded! Parameters: {sum(p.numel() for p in base_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentences — two about baseball, two about space\n",
    "sentences = [\n",
    "    \"The pitcher threw a fastball for a strike in the bottom of the ninth.\",\n",
    "    \"The batter hit a grand slam home run to win the championship game.\",\n",
    "    \"NASA launched a new satellite to orbit Mars and study its atmosphere.\",\n",
    "    \"The Hubble telescope captured images of a distant galaxy cluster.\",\n",
    "]\n",
    "\n",
    "labels = [\"baseball\", \"baseball\", \"space\", \"space\"]\n",
    "\n",
    "# Tokenize and get embeddings\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = base_model(**inputs)\n",
    "\n",
    "# Use the [CLS] token embedding as the sentence representation\n",
    "embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "print(f\"Each sentence is represented as a {embeddings.shape[1]}-dimensional vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosine-similarity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute pairwise cosine similarity\n",
    "sim_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(f\"{'':>12s}\", end=\"\")\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"  {label}-{i+1:d}\", end=\"\")\n",
    "print()\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    print(f\"{labels[i]}-{i+1:d}:     \", end=\"\")\n",
    "    for j in range(len(sentences)):\n",
    "        print(f\"  {sim_matrix[i][j]:.3f} \", end=\"\")\n",
    "    print()\n",
    "\n",
    "# Check: are same-topic sentences more similar?\n",
    "same_topic = (sim_matrix[0][1] + sim_matrix[2][3]) / 2\n",
    "diff_topic = (sim_matrix[0][2] + sim_matrix[0][3] + sim_matrix[1][2] + sim_matrix[1][3]) / 4\n",
    "\n",
    "print(f\"\\nAvg same-topic similarity:  {same_topic:.3f}\")\n",
    "print(f\"Avg cross-topic similarity: {diff_topic:.3f}\")\n",
    "print(f\"\\nThe model already groups similar topics closer together!\")\n",
    "print(\"But the gap is small — fine-tuning will make it much larger.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-insight",
   "metadata": {},
   "source": [
    "**Key insight:** The model already understands language structure. Baseball sentences are slightly\n",
    "more similar to each other than to space sentences. But the gap is small because the model wasn't\n",
    "trained for *this specific task*. Fine-tuning will amplify this signal.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-header",
   "metadata": {},
   "source": [
    "## Section 2: Zero-Shot vs Fine-Tuned\n",
    "\n",
    "Before fine-tuning, let's establish a baseline. **Zero-shot classification** uses the model's\n",
    "general knowledge to classify text into categories it has never been explicitly trained on.\n",
    "\n",
    "How well can DistilBERT classify baseball vs space posts *without any task-specific training*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zero-shot-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load a small test set for our zero-shot baseline\n",
    "categories = ['rec.sport.baseball', 'sci.space']\n",
    "test_raw = fetch_20newsgroups(\n",
    "    subset='test', categories=categories, shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "# Use a small subset for zero-shot (it's slow per-example)\n",
    "n_zero_shot = 50\n",
    "zs_texts = test_raw.data[:n_zero_shot]\n",
    "zs_labels = test_raw.target[:n_zero_shot]\n",
    "\n",
    "print(f\"Zero-shot test set: {n_zero_shot} examples\")\n",
    "print(f\"Classes: {test_raw.target_names}\")\n",
    "print(f\"  Label 0: {test_raw.target_names[0]}\")\n",
    "print(f\"  Label 1: {test_raw.target_names[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zero-shot-classify",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Zero-shot classification using DistilBERT\n",
    "# Note: this uses a different model variant optimized for NLI (natural language inference)\n",
    "print(\"Running zero-shot classification (this takes a minute)...\")\n",
    "zs_classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"typeform/distilbert-base-uncased-mnli\",\n",
    "    device=-1  # CPU\n",
    ")\n",
    "\n",
    "candidate_labels = [\"baseball\", \"space\"]\n",
    "label_map = {\"baseball\": 0, \"space\": 1}\n",
    "\n",
    "zs_predictions = []\n",
    "for text in tqdm(zs_texts, desc=\"Zero-shot\"):\n",
    "    # Truncate long texts to speed up\n",
    "    truncated = text[:512]\n",
    "    result = zs_classifier(truncated, candidate_labels=candidate_labels)\n",
    "    predicted_label = result[\"labels\"][0]\n",
    "    zs_predictions.append(label_map[predicted_label])\n",
    "\n",
    "zs_predictions = np.array(zs_predictions)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "zs_accuracy = accuracy_score(zs_labels, zs_predictions)\n",
    "print(f\"\\nZero-shot accuracy: {zs_accuracy:.3f} ({zs_accuracy*100:.1f}%)\")\n",
    "print(\"Not bad for a model that was never trained on this task!\")\n",
    "print(\"But let's see if fine-tuning can do better.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-insight",
   "metadata": {},
   "source": [
    "**Key insight:** Zero-shot classification uses general knowledge. It's decent but not great.\n",
    "The model has never seen these specific newsgroup posts or been told what \"baseball\" and \"space\"\n",
    "mean in this context. Fine-tuning teaches it YOUR data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-header",
   "metadata": {},
   "source": [
    "## Section 3: Prepare Training Data\n",
    "\n",
    "To fine-tune, we need labeled training data. We'll use the same 20 Newsgroups dataset, but now\n",
    "we'll prepare it properly for the HuggingFace Trainer:\n",
    "\n",
    "1. Load the data\n",
    "2. Tokenize with DistilBERT's tokenizer\n",
    "3. Create train/validation/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the full dataset\n",
    "categories = ['rec.sport.baseball', 'sci.space']\n",
    "train_raw = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "test_raw = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"Full training set: {len(train_raw.data)} examples\")\n",
    "print(f\"Full test set:     {len(test_raw.data)} examples\")\n",
    "\n",
    "# Use a smaller subset to keep training fast on CPU\n",
    "# In production, you'd use the full dataset\n",
    "MAX_TRAIN = 200\n",
    "MAX_TEST = 100\n",
    "\n",
    "train_texts = train_raw.data[:MAX_TRAIN]\n",
    "train_labels = train_raw.target[:MAX_TRAIN]\n",
    "\n",
    "# Split the subset into train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    ")\n",
    "\n",
    "X_test = test_raw.data[:MAX_TEST]\n",
    "y_test = test_raw.target[:MAX_TEST]\n",
    "\n",
    "print(f\"\\nSubset for CPU training:\")\n",
    "print(f\"  Train:      {len(X_train)} examples\")\n",
    "print(f\"  Validation: {len(X_val)} examples\")\n",
    "print(f\"  Test:       {len(X_test)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenize-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize all splits\n",
    "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(X_val, truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(X_test, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Show what tokenization looks like\n",
    "example_text = X_train[0][:200]\n",
    "example_tokens = tokenizer.tokenize(example_text)\n",
    "\n",
    "print(f\"Original text (first 200 chars):\")\n",
    "print(f\"  {example_text}\")\n",
    "print(f\"\\nTokenized ({len(example_tokens)} tokens):\")\n",
    "print(f\"  {example_tokens[:20]}...\")\n",
    "print(f\"\\nToken IDs (first 20):\")\n",
    "print(f\"  {train_encodings['input_ids'][0][:20]}\")\n",
    "print(f\"\\nThe tokenizer splits text into subword tokens and converts them to integer IDs.\")\n",
    "print(f\"The model only sees numbers — never raw text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for our tokenized newsgroup data.\"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = NewsGroupDataset(train_encodings, y_train.tolist())\n",
    "val_dataset = NewsGroupDataset(val_encodings, y_val.tolist())\n",
    "test_dataset = NewsGroupDataset(test_encodings, y_test.tolist())\n",
    "\n",
    "print(f\"Datasets created:\")\n",
    "print(f\"  train_dataset: {len(train_dataset)} examples\")\n",
    "print(f\"  val_dataset:   {len(val_dataset)} examples\")\n",
    "print(f\"  test_dataset:  {len(test_dataset)} examples\")\n",
    "\n",
    "# Peek at one example\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nSample item keys: {list(sample.keys())}\")\n",
    "print(f\"  input_ids shape: {sample['input_ids'].shape}\")\n",
    "print(f\"  label: {sample['labels'].item()} ({categories[sample['labels'].item()]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-insight",
   "metadata": {},
   "source": [
    "**Key insight:** The tokenizer converts raw text into integer IDs that the model understands.\n",
    "DistilBERT uses *subword tokenization* — common words stay whole, rare words get split into pieces.\n",
    "This is why it can handle any text, even words it's never seen before.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-header",
   "metadata": {},
   "source": [
    "## Section 4: Fine-Tune DistilBERT\n",
    "\n",
    "Now the main event. We'll:\n",
    "1. Load `DistilBertForSequenceClassification` — a DistilBERT with a classification head on top\n",
    "2. Set training arguments (epochs, learning rate, batch size)\n",
    "3. Train with the HuggingFace `Trainer`\n",
    "4. Watch the loss decrease\n",
    "\n",
    "On CPU, this takes about 5-10 minutes with our small dataset. On GPU, it would take seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Load DistilBERT with a classification head (2 classes)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "print(f\"Model loaded with classification head\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Define a function to compute metrics during training\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments — tuned for CPU speed\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"none\",  # Disable W&B/MLflow reporting\n",
    "    no_cuda=True,       # Force CPU\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Starting fine-tuning (3 epochs on CPU — ~5-10 minutes)...\")\n",
    "print(\"Watch the training loss decrease!\\n\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"  Total steps: {train_result.global_step}\")\n",
    "print(f\"  Final training loss: {train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "print(\"Evaluating on test set...\\n\")\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "print(f\"Test Results:\")\n",
    "print(f\"  Accuracy: {test_results['eval_accuracy']:.3f} ({test_results['eval_accuracy']*100:.1f}%)\")\n",
    "print(f\"  F1 Score: {test_results['eval_f1']:.3f}\")\n",
    "print(f\"  Loss:     {test_results['eval_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-insight",
   "metadata": {},
   "source": [
    "**Key insight:** The training loss should decrease steadily over the 3 epochs. The model is\n",
    "adjusting its 66 million parameters — but mostly the classification head and the top layers.\n",
    "The lower layers (which capture general language understanding) change very little.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5-header",
   "metadata": {},
   "source": [
    "## Section 5: Compare Results\n",
    "\n",
    "Time for the payoff. Let's compare zero-shot vs fine-tuned side by side, look at the\n",
    "classification report, plot the confusion matrix, and visualize the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fine-tuned predictions on the same test set\n",
    "ft_output = trainer.predict(test_dataset)\n",
    "ft_predictions = np.argmax(ft_output.predictions, axis=-1)\n",
    "ft_accuracy = accuracy_score(y_test, ft_predictions)\n",
    "\n",
    "# Build comparison table\n",
    "print(\"=\" * 50)\n",
    "print(\"  Zero-Shot vs Fine-Tuned Comparison\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\")\n",
    "print(f\"  {'Method':<20s} {'Accuracy':>10s}\")\n",
    "print(f\"  {'-'*20} {'-'*10}\")\n",
    "print(f\"  {'Zero-shot':<20s} {zs_accuracy:>9.1%}\")\n",
    "print(f\"  {'Fine-tuned':<20s} {ft_accuracy:>9.1%}\")\n",
    "print(f\"  {'-'*20} {'-'*10}\")\n",
    "print(f\"  {'Improvement':<20s} {(ft_accuracy - zs_accuracy):>+9.1%}\")\n",
    "print(f\"\")\n",
    "print(f\"A few hundred examples and 3 epochs transformed\")\n",
    "print(f\"a general model into a specialist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classification-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Classification report\n",
    "target_names = ['rec.sport.baseball', 'sci.space']\n",
    "print(\"Fine-Tuned Model — Classification Report:\")\n",
    "print(classification_report(y_test, ft_predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, ft_predictions)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=target_names,\n",
    "            yticklabels=target_names, ax=ax)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.set_title('Fine-Tuned DistilBERT — Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5-middle",
   "metadata": {},
   "source": [
    "### Section 5b: Explore Embeddings with t-SNE\n",
    "\n",
    "Let's extract the embeddings from the fine-tuned model and visualize them in 2D.\n",
    "If fine-tuning worked, we should see clear separation between the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Extract embeddings from the fine-tuned model\n",
    "# We need to get the hidden states before the classification head\n",
    "model.eval()\n",
    "all_embeddings = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):\n",
    "        item = test_dataset[i]\n",
    "        input_ids = item['input_ids'].unsqueeze(0)\n",
    "        attention_mask = item['attention_mask'].unsqueeze(0)\n",
    "        \n",
    "        # Get hidden states from the base model (before classification head)\n",
    "        outputs = model.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "        \n",
    "        all_embeddings.append(cls_embedding)\n",
    "        all_labels.append(item['labels'].item())\n",
    "\n",
    "all_embeddings = np.array(all_embeddings)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "print(f\"Extracted {len(all_embeddings)} embeddings of dimension {all_embeddings.shape[1]}\")\n",
    "\n",
    "# Reduce to 2D with t-SNE\n",
    "print(\"Running t-SNE dimensionality reduction...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(all_embeddings) - 1))\n",
    "embeddings_2d = tsne.fit_transform(all_embeddings)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-tsne",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot t-SNE visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "colors = ['#2196F3', '#FF9800']  # Blue for baseball, orange for space\n",
    "for label_idx in [0, 1]:\n",
    "    mask = all_labels == label_idx\n",
    "    ax.scatter(\n",
    "        embeddings_2d[mask, 0],\n",
    "        embeddings_2d[mask, 1],\n",
    "        c=colors[label_idx],\n",
    "        label=target_names[label_idx],\n",
    "        alpha=0.7,\n",
    "        s=50,\n",
    "        edgecolors='white',\n",
    "        linewidth=0.5\n",
    "    )\n",
    "\n",
    "ax.set_title('Fine-Tuned DistilBERT Embeddings (t-SNE)', fontsize=14)\n",
    "ax.set_xlabel('t-SNE dimension 1')\n",
    "ax.set_ylabel('t-SNE dimension 2')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The clear separation shows the model learned distinct representations\")\n",
    "print(\"for each class. These learned embeddings are what make search,\")\n",
    "print(\"recommendations, and RAG systems work.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5-insight",
   "metadata": {},
   "source": [
    "**Key insight:** The t-SNE plot should show two clearly separated clusters. This means the\n",
    "fine-tuned model has learned to push baseball posts and space posts into different regions of\n",
    "embedding space. The better the separation, the easier classification becomes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6-header",
   "metadata": {},
   "source": [
    "## Section 6: Save and Track\n",
    "\n",
    "A fine-tuned model is only useful if you can reload it later. Let's save it, check the file sizes,\n",
    "and record the full experiment lineage — everything someone would need to reproduce this result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "save_dir = \"./fine_tuned_distilbert\"\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "# Check what files were created\n",
    "print(\"Saved model files:\")\n",
    "total_size = 0\n",
    "for fname in sorted(os.listdir(save_dir)):\n",
    "    fpath = os.path.join(save_dir, fname)\n",
    "    size = os.path.getsize(fpath)\n",
    "    total_size += size\n",
    "    size_mb = size / (1024 * 1024)\n",
    "    print(f\"  {fname:<40s} {size_mb:>8.2f} MB\")\n",
    "\n",
    "print(f\"  {'':->40s} {'':->8s}---\")\n",
    "print(f\"  {'TOTAL':<40s} {total_size / (1024 * 1024):>8.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reload-and-verify",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Reload the model from disk\n",
    "loaded_model = DistilBertForSequenceClassification.from_pretrained(save_dir)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "loaded_model.eval()\n",
    "\n",
    "# Test on new examples\n",
    "test_sentences = [\n",
    "    \"The shortstop made a diving catch to end the inning.\",\n",
    "    \"SpaceX successfully landed its booster rocket after launch.\",\n",
    "    \"The umpire called strike three on the final pitch.\",\n",
    "    \"The Mars rover collected soil samples from the crater.\",\n",
    "]\n",
    "\n",
    "inputs = loaded_tokenizer(test_sentences, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = loaded_model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    probabilities = torch.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "print(\"Predictions from the reloaded model:\\n\")\n",
    "for text, pred, proba in zip(test_sentences, predictions, probabilities):\n",
    "    label = target_names[pred.item()]\n",
    "    confidence = proba[pred.item()].item() * 100\n",
    "    print(f\"  [{label:>22s}] ({confidence:.0f}%) \\\"{text[:55]}...\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experiment-log",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Record full experiment lineage\n",
    "experiment_log = {\n",
    "    \"experiment\": \"Lab 09 — Fine-Tune DistilBERT\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"base_model\": \"distilbert-base-uncased\",\n",
    "    \"task\": \"binary text classification (baseball vs space)\",\n",
    "    \"data\": {\n",
    "        \"source\": \"20newsgroups (scikit-learn)\",\n",
    "        \"categories\": categories,\n",
    "        \"train_size\": len(X_train),\n",
    "        \"val_size\": len(X_val),\n",
    "        \"test_size\": len(X_test),\n",
    "        \"max_length\": 128,\n",
    "    },\n",
    "    \"hyperparameters\": {\n",
    "        \"epochs\": 3,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 16,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"optimizer\": \"AdamW\",\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"zero_shot_accuracy\": round(zs_accuracy, 3),\n",
    "        \"fine_tuned_accuracy\": round(ft_accuracy, 3),\n",
    "        \"fine_tuned_f1\": round(test_results['eval_f1'], 3),\n",
    "        \"improvement\": round(ft_accuracy - zs_accuracy, 3),\n",
    "    },\n",
    "    \"model_path\": save_dir,\n",
    "    \"model_size_mb\": round(total_size / (1024 * 1024), 1),\n",
    "}\n",
    "\n",
    "# Save the experiment log\n",
    "log_path = \"./experiment_log.json\"\n",
    "with open(log_path, \"w\") as f:\n",
    "    json.dump(experiment_log, f, indent=2)\n",
    "\n",
    "print(\"Experiment Log:\")\n",
    "print(json.dumps(experiment_log, indent=2))\n",
    "print(f\"\\nSaved to: {log_path}\")\n",
    "print(f\"\\nFull lineage: base model + training data + hyperparameters = your fine-tuned model.\")\n",
    "print(f\"Anyone can reproduce this result with this log.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up saved files (optional — comment out if you want to keep them)\n",
    "import shutil\n",
    "\n",
    "if os.path.exists(save_dir):\n",
    "    shutil.rmtree(save_dir)\n",
    "    print(f\"Cleaned up {save_dir}\")\n",
    "\n",
    "if os.path.exists(log_path):\n",
    "    os.remove(log_path)\n",
    "    print(f\"Cleaned up {log_path}\")\n",
    "\n",
    "if os.path.exists(\"./results\"):\n",
    "    shutil.rmtree(\"./results\")\n",
    "    print(\"Cleaned up ./results\")\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "You just fine-tuned a language model. Here's what you now know:\n",
    "\n",
    "| Concept | What You Learned |\n",
    "|---------|------------------|\n",
    "| **Pre-trained knowledge** | DistilBERT already understands language — it groups similar topics together |\n",
    "| **Zero-shot baseline** | General knowledge gets you decent accuracy without any training |\n",
    "| **Fine-tuning** | A few hundred examples + 3 epochs dramatically improves task-specific performance |\n",
    "| **Transfer learning** | You're adapting existing knowledge, not learning from scratch |\n",
    "| **Embeddings** | t-SNE shows clear class separation after fine-tuning |\n",
    "| **Model lineage** | Base model + data + hyperparameters = reproducible fine-tuned model |\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **ML Lab 10**, you'll build a RAG pipeline — combining retrieval with generation to answer\n",
    "questions using your own documents. The embeddings you learned about here are exactly what make\n",
    "RAG work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
