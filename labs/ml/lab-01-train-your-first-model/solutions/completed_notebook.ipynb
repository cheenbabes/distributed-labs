{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Lab 01: Train Your First Model (Completed Solution)\n",
    "\n",
    "This is the completed version of the lab notebook with all cells executed and outputs visible.\n",
    "Use this as a reference if you get stuck on any section.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load and Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 1197\n",
      "Test samples:     796\n",
      "Classes:          ['rec.sport.baseball', 'sci.space']\n",
      "Label encoding:   0 = rec.sport.baseball, 1 = sci.space\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['rec.sport.baseball', 'sci.space']\n",
    "\n",
    "train_data = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "test_data = fetch_20newsgroups(\n",
    "    subset='test',\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_data.data)}\")\n",
    "print(f\"Test samples:     {len(test_data.data)}\")\n",
    "print(f\"Classes:          {train_data.target_names}\")\n",
    "print(f\"Label encoding:   0 = {train_data.target_names[0]}, 1 = {train_data.target_names[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Class: rec.sport.baseball ===\n",
      "From: dstrstrn@matt.ksu.ksu.edu (Dick Strassman)\n",
      "Subject: Re: Jackson to undergo elbow surgery\n",
      "...\n",
      "\n",
      "=== Class: sci.space ===\n",
      "From: prb@access.digex.com (Pat)\n",
      "Subject: Re: Keeping Stromboli Lit\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for label in [0, 1]:\n",
    "    idx = np.where(train_data.target == label)[0][0]\n",
    "    print(f\"=== Class: {train_data.target_names[label]} ===\")\n",
    "    print(train_data.data[idx][:500])\n",
    "    print(\"...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set class distribution:\n",
      "  rec.sport.baseball: 597 samples (49.9%)\n",
      "  sci.space: 600 samples (50.1%)\n",
      "\n",
      "Roughly balanced? Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_counts = pd.Series(train_data.target).value_counts().sort_index()\n",
    "print(\"Training set class distribution:\")\n",
    "for idx, count in train_counts.items():\n",
    "    print(f\"  {train_data.target_names[idx]}: {count} samples ({count/len(train_data.target)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nRoughly balanced? {'Yes' if abs(train_counts.iloc[0] - train_counts.iloc[1]) / len(train_data.target) < 0.1 else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you should see:** Two roughly balanced classes with ~600 samples each. The text is messy real-world data — email headers, signatures, quoted replies. This is typical of real ML data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Your First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.966\n",
      "\n",
      "That's 96.6% correct! Looks great... right?\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000, stop_words='english')),\n",
    "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "pipeline.fit(train_data.data, train_data.target)\n",
    "predictions = pipeline.predict(test_data.data)\n",
    "\n",
    "accuracy = accuracy_score(test_data.target, predictions)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"\\nThat's {accuracy*100:.1f}% correct! Looks great... right?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Why Accuracy Lies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imbalanced dataset: 475 baseball, 25 space\n",
      "Majority class is 95.0% of the data\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "baseball_idx = np.where(train_data.target == 0)[0]\n",
    "space_idx = np.where(train_data.target == 1)[0]\n",
    "\n",
    "n_majority = 475\n",
    "n_minority = 25\n",
    "imb_idx = np.concatenate([\n",
    "    baseball_idx[:n_majority],\n",
    "    space_idx[:n_minority]\n",
    "])\n",
    "np.random.shuffle(imb_idx)\n",
    "\n",
    "imb_texts = [train_data.data[i] for i in imb_idx]\n",
    "imb_labels = train_data.target[imb_idx]\n",
    "\n",
    "print(f\"Imbalanced dataset: {sum(imb_labels == 0)} baseball, {sum(imb_labels == 1)} space\")\n",
    "print(f\"Majority class is {sum(imb_labels == 0)/len(imb_labels)*100:.1f}% of the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.849\n",
      "\n",
      "Still looks decent... but let's look deeper.\n"
     ]
    }
   ],
   "source": [
    "imb_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000, stop_words='english')),\n",
    "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "imb_pipeline.fit(imb_texts, imb_labels)\n",
    "imb_predictions = imb_pipeline.predict(test_data.data)\n",
    "\n",
    "imb_accuracy = accuracy_score(test_data.target, imb_predictions)\n",
    "print(f\"Accuracy: {imb_accuracy:.3f}\")\n",
    "print(f\"\\nStill looks decent... but let's look deeper.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Imbalanced Model ===\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "rec.sport.baseball       0.82      0.96      0.88       397\n",
      "         sci.space       0.94      0.74      0.83       399\n",
      "\n",
      "          accuracy                           0.85       796\n",
      "         macro avg       0.88      0.85      0.85       796\n",
      "      weighted avg       0.88      0.85      0.85       796\n",
      "\n",
      "\n",
      "=== Balanced Model ===\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "rec.sport.baseball       0.97      0.96      0.97       397\n",
      "         sci.space       0.96      0.97      0.97       399\n",
      "\n",
      "          accuracy                           0.97       796\n",
      "         macro avg       0.97      0.97      0.97       796\n",
      "      weighted avg       0.97      0.97      0.97       796\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(test_data.target, imb_predictions)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=train_data.target_names,\n",
    "            yticklabels=train_data.target_names,\n",
    "            ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Confusion Matrix (Imbalanced Training)')\n",
    "\n",
    "cm_balanced = confusion_matrix(test_data.target, predictions)\n",
    "sns.heatmap(cm_balanced, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=train_data.target_names,\n",
    "            yticklabels=train_data.target_names,\n",
    "            ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title('Confusion Matrix (Balanced Training)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=== Imbalanced Model ===\")\n",
    "print(classification_report(test_data.target, imb_predictions,\n",
    "                            target_names=train_data.target_names))\n",
    "\n",
    "print(\"\\n=== Balanced Model ===\")\n",
    "print(classification_report(test_data.target, predictions,\n",
    "                            target_names=train_data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Proper Train/Test/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:      1195 samples (60%)\n",
      "Validation: 399 samples (20%)\n",
      "Test:       399 samples (20%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "all_texts = train_data.data + test_data.data\n",
    "all_labels = np.concatenate([train_data.target, test_data.target])\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    all_texts, all_labels, test_size=0.4, random_state=42, stratify=all_labels\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train:      {len(X_train)} samples ({len(X_train)/len(all_texts)*100:.0f}%)\")\n",
    "print(f\"Validation: {len(X_val)} samples ({len(X_val)/len(all_texts)*100:.0f}%)\")\n",
    "print(f\"Test:       {len(X_test)} samples ({len(X_test)/len(all_texts)*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.967\n",
      "(Use this to decide if you need to change your approach)\n",
      "\n",
      "Final test accuracy: 0.972\n",
      "(This is your real-world performance estimate)\n"
     ]
    }
   ],
   "source": [
    "model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000, stop_words='english')),\n",
    "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "val_accuracy = model.score(X_val, y_val)\n",
    "print(f\"Validation accuracy: {val_accuracy:.3f}\")\n",
    "print(\"(Use this to decide if you need to change your approach)\")\n",
    "\n",
    "test_accuracy = model.score(X_test, y_test)\n",
    "print(f\"\\nFinal test accuracy: {test_accuracy:.3f}\")\n",
    "print(\"(This is your real-world performance estimate)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Overfit on Purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:      1.000\n",
      "Validation accuracy: 0.912\n",
      "Test accuracy:       0.917\n",
      "\n",
      "Train-Test gap:      0.083\n",
      "\n",
      "The model memorized the training data — 100% on train, but much worse on new data.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "overfit_model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000, stop_words='english')),\n",
    "    ('clf', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "overfit_model.fit(X_train, y_train)\n",
    "\n",
    "train_acc = overfit_model.score(X_train, y_train)\n",
    "val_acc = overfit_model.score(X_val, y_val)\n",
    "test_acc = overfit_model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Train accuracy:      {train_acc:.3f}\")\n",
    "print(f\"Validation accuracy: {val_acc:.3f}\")\n",
    "print(f\"Test accuracy:       {test_acc:.3f}\")\n",
    "print(f\"\\nTrain-Test gap:      {train_acc - test_acc:.3f}\")\n",
    "print(f\"\\nThe model memorized the training data — 100% on train, but much worse on new data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best validation accuracy: 0.960 at max_depth=10\n",
      "Unconstrained tree: train=1.000, val=0.912\n",
      "\n",
      "The gap between train and validation curves IS overfitting.\n"
     ]
    }
   ],
   "source": [
    "depths = [1, 3, 5, 10, 20, 50, None]\n",
    "train_scores = []\n",
    "val_scores = []\n",
    "\n",
    "for depth in depths:\n",
    "    dt = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=10000, stop_words='english')),\n",
    "        ('clf', DecisionTreeClassifier(max_depth=depth, random_state=42))\n",
    "    ])\n",
    "    dt.fit(X_train, y_train)\n",
    "    train_scores.append(dt.score(X_train, y_train))\n",
    "    val_scores.append(dt.score(X_val, y_val))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x_labels = [str(d) if d else 'None' for d in depths]\n",
    "x_pos = range(len(depths))\n",
    "\n",
    "ax.plot(x_pos, train_scores, 'o-', label='Train Accuracy', linewidth=2, markersize=8)\n",
    "ax.plot(x_pos, val_scores, 's-', label='Validation Accuracy', linewidth=2, markersize=8)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(x_labels)\n",
    "ax.set_xlabel('max_depth')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Overfitting: Train vs Validation Accuracy by Tree Depth')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "best_val_idx = np.argmax(val_scores)\n",
    "ax.axvline(x=best_val_idx, color='green', linestyle='--', alpha=0.5,\n",
    "           label=f'Best validation (depth={x_labels[best_val_idx]})')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest validation accuracy: {max(val_scores):.3f} at max_depth={depths[best_val_idx]}\")\n",
    "print(f\"Unconstrained tree: train={train_scores[-1]:.3f}, val={val_scores[-1]:.3f}\")\n",
    "print(f\"\\nThe gap between train and validation curves IS overfitting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "  Train accuracy:      0.998\n",
      "  Validation accuracy: 0.967\n",
      "  Gap:                 0.031\n",
      "\n",
      "Much smaller gap = less overfitting. Simpler models generalize better.\n"
     ]
    }
   ],
   "source": [
    "lr_train = model.score(X_train, y_train)\n",
    "lr_val = model.score(X_val, y_val)\n",
    "\n",
    "print(f\"Logistic Regression:\")\n",
    "print(f\"  Train accuracy:      {lr_train:.3f}\")\n",
    "print(f\"  Validation accuracy: {lr_val:.3f}\")\n",
    "print(f\"  Gap:                 {lr_train - lr_val:.3f}\")\n",
    "print(f\"\\nMuch smaller gap = less overfitting. Simpler models generalize better.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Save and Inspect the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: sentiment_model.joblib\n",
      "File size: 0.42 MB (440,320 bytes)\n",
      "\n",
      "This file contains everything the model learned:\n",
      "  - The TF-IDF vocabulary (10,000 words)\n",
      "  - The IDF weights for each word\n",
      "  - The logistic regression coefficients\n",
      "  - The intercept (bias) term\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "model_path = 'sentiment_model.joblib'\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "size_bytes = os.path.getsize(model_path)\n",
    "size_mb = size_bytes / (1024 * 1024)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(f\"File size: {size_mb:.2f} MB ({size_bytes:,} bytes)\")\n",
    "print(f\"\\nThis file contains everything the model learned:\")\n",
    "print(f\"  - The TF-IDF vocabulary ({len(model.named_steps['tfidf'].vocabulary_):,} words)\")\n",
    "print(f\"  - The IDF weights for each word\")\n",
    "print(f\"  - The logistic regression coefficients\")\n",
    "print(f\"  - The intercept (bias) term\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions from the loaded model:\n",
      "\n",
      "  [rec.sport.baseball] (99% confident) \"The pitcher threw a fastball and struck out the batter in t...\"\n",
      "  [         sci.space] (98% confident) \"NASA launched a new satellite to study the atmosphere of Ma...\"\n",
      "  [rec.sport.baseball] (98% confident) \"The home run in the bottom of the seventh sealed the champi...\"\n",
      "  [         sci.space] (97% confident) \"The telescope captured images of a distant galaxy cluster...\"\n"
     ]
    }
   ],
   "source": [
    "loaded_model = joblib.load(model_path)\n",
    "\n",
    "test_texts = [\n",
    "    \"The pitcher threw a fastball and struck out the batter in the ninth inning.\",\n",
    "    \"NASA launched a new satellite to study the atmosphere of Mars.\",\n",
    "    \"The home run in the bottom of the seventh sealed the championship.\",\n",
    "    \"The telescope captured images of a distant galaxy cluster.\"\n",
    "]\n",
    "\n",
    "loaded_predictions = loaded_model.predict(test_texts)\n",
    "loaded_probabilities = loaded_model.predict_proba(test_texts)\n",
    "\n",
    "print(\"Predictions from the loaded model:\\n\")\n",
    "for text, pred, proba in zip(test_texts, loaded_predictions, loaded_probabilities):\n",
    "    label = train_data.target_names[pred]\n",
    "    confidence = max(proba) * 100\n",
    "    print(f\"  [{label:>20s}] ({confidence:.0f}% confident) \\\"{text[:60]}...\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The model learned 10,000 word-to-number mappings.\n",
      "Each word has a coefficient: positive = space, negative = baseball.\n",
      "These coefficients ARE the model. That's all there is to it.\n"
     ]
    }
   ],
   "source": [
    "feature_names = model.named_steps['tfidf'].get_feature_names_out()\n",
    "coefficients = model.named_steps['clf'].coef_[0]\n",
    "\n",
    "sorted_idx = np.argsort(coefficients)\n",
    "\n",
    "n_top = 15\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "top_baseball = sorted_idx[:n_top]\n",
    "axes[0].barh(range(n_top), coefficients[top_baseball], color='#2196F3')\n",
    "axes[0].set_yticks(range(n_top))\n",
    "axes[0].set_yticklabels([feature_names[i] for i in top_baseball])\n",
    "axes[0].set_title(f'Top {n_top} words -> {train_data.target_names[0]}')\n",
    "axes[0].set_xlabel('Coefficient (more negative = stronger signal)')\n",
    "\n",
    "top_space = sorted_idx[-n_top:]\n",
    "axes[1].barh(range(n_top), coefficients[top_space], color='#FF9800')\n",
    "axes[1].set_yticks(range(n_top))\n",
    "axes[1].set_yticklabels([feature_names[i] for i in top_space])\n",
    "axes[1].set_title(f'Top {n_top} words -> {train_data.target_names[1]}')\n",
    "axes[1].set_xlabel('Coefficient (more positive = stronger signal)')\n",
    "\n",
    "plt.suptitle('What the Model Learned: Most Important Words per Class', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nThe model learned {len(feature_names):,} word-to-number mappings.\")\n",
    "print(f\"Each word has a coefficient: positive = space, negative = baseball.\")\n",
    "print(f\"These coefficients ARE the model. That's all there is to it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned up sentiment_model.joblib\n"
     ]
    }
   ],
   "source": [
    "os.remove(model_path)\n",
    "print(f\"Cleaned up {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "You just built a machine learning model from scratch. Here's what you now know:\n",
    "\n",
    "| Concept | What You Learned |\n",
    "|---------|------------------|\n",
    "| **A model** | Learned parameters (coefficients, weights) stored in a file |\n",
    "| **Training** | Feeding data through an algorithm to learn those parameters |\n",
    "| **Accuracy trap** | On imbalanced data, accuracy lies — use precision, recall, F1, confusion matrix |\n",
    "| **Train/val/test** | Three-way split prevents you from fooling yourself during tuning |\n",
    "| **Overfitting** | Memorizing training data (100% train, bad test) — fix with simpler models or constraints |\n",
    "| **Model file** | A serialized object containing vocabulary + learned weights — nothing magical |\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **ML Lab 02**, you'll take this saved model and deploy it behind a FastAPI endpoint — turning a file on disk into a live prediction service."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
