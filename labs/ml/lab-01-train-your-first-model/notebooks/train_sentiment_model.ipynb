{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Lab 01: Train Your First Model\n",
    "\n",
    "You've heard \"machine learning model\" a thousand times. But what actually *is* a model?\n",
    "\n",
    "In this lab, you'll find out by building one from scratch. You'll train a text classifier,\n",
    "discover why accuracy is a trap, learn what overfitting looks like, and save your model to\n",
    "a file you can inspect. By the end, there's no more mystery â€” a model is just learned\n",
    "parameters stored on disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Load and Explore the Data\n",
    "\n",
    "We'll use the **20 Newsgroups** dataset, which is built into scikit-learn (no downloads needed).\n",
    "It contains ~20,000 newsgroup posts across 20 topics. We'll pick two for binary classification:\n",
    "- `rec.sport.baseball` (baseball discussions)\n",
    "- `sci.space` (space science discussions)\n",
    "\n",
    "Our goal: given a text post, predict which topic it belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['rec.sport.baseball', 'sci.space']\n",
    "\n",
    "train_data = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "test_data = fetch_20newsgroups(\n",
    "    subset='test',\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_data.data)}\")\n",
    "print(f\"Test samples:     {len(test_data.data)}\")\n",
    "print(f\"Classes:          {train_data.target_names}\")\n",
    "print(f\"Label encoding:   0 = {train_data.target_names[0]}, 1 = {train_data.target_names[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at one example from each class\n",
    "import numpy as np\n",
    "\n",
    "for label in [0, 1]:\n",
    "    idx = np.where(train_data.target == label)[0][0]\n",
    "    print(f\"=== Class: {train_data.target_names[label]} ===\")\n",
    "    print(train_data.data[idx][:500])\n",
    "    print(\"...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class balance\n",
    "import pandas as pd\n",
    "\n",
    "train_counts = pd.Series(train_data.target).value_counts().sort_index()\n",
    "print(\"Training set class distribution:\")\n",
    "for idx, count in train_counts.items():\n",
    "    print(f\"  {train_data.target_names[idx]}: {count} samples ({count/len(train_data.target)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nRoughly balanced? {'Yes' if abs(train_counts.iloc[0] - train_counts.iloc[1]) / len(train_data.target) < 0.1 else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you should see:** Two roughly balanced classes with ~600 samples each. The text is messy real-world data â€” email headers, signatures, quoted replies. This is typical of real ML data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Your First Model\n",
    "\n",
    "To classify text, we need to convert words into numbers. We'll use **TF-IDF** (Term Frequency-Inverse Document Frequency), which scores each word by:\n",
    "- How often it appears in *this* document (TF â€” more = higher)\n",
    "- How rare it is across *all* documents (IDF â€” rarer = higher)\n",
    "\n",
    "Then we'll feed those numbers into **Logistic Regression**, a simple but powerful classifier.\n",
    "\n",
    "scikit-learn's `Pipeline` chains these two steps together so we can treat them as one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Build the pipeline: text -> TF-IDF vectors -> Logistic Regression\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000, stop_words='english')),\n",
    "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# Train on training data\n",
    "pipeline.fit(train_data.data, train_data.target)\n",
    "\n",
    "# Predict on test data\n",
    "predictions = pipeline.predict(test_data.data)\n",
    "\n",
    "accuracy = accuracy_score(test_data.target, predictions)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"\\nThat's {accuracy*100:.1f}% correct! Looks great... right?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feels good, doesn't it?** 96%+ accuracy on your first model! But before you celebrate, let's see why accuracy can lie to you.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Why Accuracy Lies\n",
    "\n",
    "Imagine a dataset where 95% of samples are class A and 5% are class B.\n",
    "A model that *always* predicts class A would score 95% accuracy â€” without learning anything.\n",
    "\n",
    "Let's prove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create an imbalanced dataset: 95% baseball, 5% space\n",
    "np.random.seed(42)\n",
    "\n",
    "# Get all baseball (class 0) and space (class 1) samples\n",
    "baseball_idx = np.where(train_data.target == 0)[0]\n",
    "space_idx = np.where(train_data.target == 1)[0]\n",
    "\n",
    "# Take 950 baseball samples and 50 space samples\n",
    "n_majority = 475\n",
    "n_minority = 25\n",
    "imb_idx = np.concatenate([\n",
    "    baseball_idx[:n_majority],\n",
    "    space_idx[:n_minority]\n",
    "])\n",
    "np.random.shuffle(imb_idx)\n",
    "\n",
    "imb_texts = [train_data.data[i] for i in imb_idx]\n",
    "imb_labels = train_data.target[imb_idx]\n",
    "\n",
    "print(f\"Imbalanced dataset: {sum(imb_labels == 0)} baseball, {sum(imb_labels == 1)} space\")\n",
    "print(f\"Majority class is {sum(imb_labels == 0)/len(imb_labels)*100:.1f}% of the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on the imbalanced data\n",
    "imb_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000, stop_words='english')),\n",
    "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "imb_pipeline.fit(imb_texts, imb_labels)\n",
    "imb_predictions = imb_pipeline.predict(test_data.data)\n",
    "\n",
    "imb_accuracy = accuracy_score(test_data.target, imb_predictions)\n",
    "print(f\"Accuracy: {imb_accuracy:.3f}\")\n",
    "print(f\"\\nStill looks decent... but let's look deeper.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The confusion matrix reveals the truth\n",
    "cm = confusion_matrix(test_data.target, imb_predictions)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion matrix heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=train_data.target_names,\n",
    "            yticklabels=train_data.target_names,\n",
    "            ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Confusion Matrix (Imbalanced Training)')\n",
    "\n",
    "# Compare with balanced model\n",
    "cm_balanced = confusion_matrix(test_data.target, predictions)\n",
    "sns.heatmap(cm_balanced, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=train_data.target_names,\n",
    "            yticklabels=train_data.target_names,\n",
    "            ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title('Confusion Matrix (Balanced Training)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The full classification report tells the real story\n",
    "print(\"=== Imbalanced Model ===\")\n",
    "print(classification_report(test_data.target, imb_predictions,\n",
    "                            target_names=train_data.target_names))\n",
    "\n",
    "print(\"\\n=== Balanced Model ===\")\n",
    "print(classification_report(test_data.target, predictions,\n",
    "                            target_names=train_data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key insight:** The imbalanced model has high accuracy but terrible **recall** for the minority class. The confusion matrix shows it barely identifies space posts correctly.\n",
    "\n",
    "**Metrics to know:**\n",
    "- **Precision**: Of all the samples predicted as class X, how many actually were? (\"When I say space, am I right?\")\n",
    "- **Recall**: Of all actual class X samples, how many did I find? (\"Did I find all the space posts?\")\n",
    "- **F1**: Harmonic mean of precision and recall â€” one number that balances both\n",
    "\n",
    "**Rule of thumb:** If your classes are imbalanced, *never* trust accuracy alone. Always check the confusion matrix.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Proper Train/Test/Validation Split\n",
    "\n",
    "So far we've used two sets: train and test. But there's a subtle problem.\n",
    "\n",
    "When you tune your model (change parameters, try different algorithms) based on test set results,\n",
    "you're *leaking information* from the test set into your decisions. Your test set is no longer truly \"unseen.\"\n",
    "\n",
    "The fix: **three-way split**.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     All Data                                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚    Training (60%)      â”‚  Val (20%)  â”‚    Test (20%)        â”‚\n",
    "â”‚    Learn patterns      â”‚  Tune here  â”‚    Final eval only   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "- **Train:** Model learns from this\n",
    "- **Validation:** You use this to compare models and tune parameters\n",
    "- **Test:** Touch this ONCE at the very end for your final score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Combine all data first, then split three ways\n",
    "all_texts = train_data.data + test_data.data\n",
    "all_labels = np.concatenate([train_data.target, test_data.target])\n",
    "\n",
    "# 60% train, 20% validation, 20% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    all_texts, all_labels, test_size=0.4, random_state=42, stratify=all_labels\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train:      {len(X_train)} samples ({len(X_train)/len(all_texts)*100:.0f}%)\")\n",
    "print(f\"Validation: {len(X_val)} samples ({len(X_val)/len(all_texts)*100:.0f}%)\")\n",
    "print(f\"Test:       {len(X_test)} samples ({len(X_test)/len(all_texts)*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The proper workflow: train, validate, then test once\n",
    "\n",
    "# Step 1: Train the model\n",
    "model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000, stop_words='english')),\n",
    "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 2: Evaluate on validation set (use this to tune)\n",
    "val_accuracy = model.score(X_val, y_val)\n",
    "print(f\"Validation accuracy: {val_accuracy:.3f}\")\n",
    "print(\"(Use this to decide if you need to change your approach)\")\n",
    "\n",
    "# Step 3: Final evaluation on test set (do this ONCE)\n",
    "test_accuracy = model.score(X_test, y_test)\n",
    "print(f\"\\nFinal test accuracy: {test_accuracy:.3f}\")\n",
    "print(\"(This is your real-world performance estimate)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why does this matter?** In practice, if you check your test score after every change, you'll unconsciously optimize for the test set. The validation set gives you a \"safe\" set to iterate on without contaminating your final evaluation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Overfit on Purpose\n",
    "\n",
    "**Overfitting** is when a model memorizes the training data instead of learning general patterns.\n",
    "It gets perfect scores on training data but fails on new data.\n",
    "\n",
    "Let's make this happen intentionally with a Decision Tree that has no limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# An unconstrained Decision Tree will memorize everything\n",
    "overfit_model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000, stop_words='english')),\n",
    "    ('clf', DecisionTreeClassifier(random_state=42))  # No max_depth â€” it will grow until it memorizes\n",
    "])\n",
    "\n",
    "overfit_model.fit(X_train, y_train)\n",
    "\n",
    "train_acc = overfit_model.score(X_train, y_train)\n",
    "val_acc = overfit_model.score(X_val, y_val)\n",
    "test_acc = overfit_model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Train accuracy:      {train_acc:.3f}\")\n",
    "print(f\"Validation accuracy: {val_acc:.3f}\")\n",
    "print(f\"Test accuracy:       {test_acc:.3f}\")\n",
    "print(f\"\\nTrain-Test gap:      {train_acc - test_acc:.3f}\")\n",
    "print(f\"\\n{'ðŸš¨ OVERFITTING!' if train_acc - test_acc > 0.1 else 'Looks OK'}\")\n",
    "print(\"The model memorized the training data â€” 100% on train, but much worse on new data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix it: constrain the tree\n",
    "depths = [1, 3, 5, 10, 20, 50, None]  # None = no limit\n",
    "train_scores = []\n",
    "val_scores = []\n",
    "\n",
    "for depth in depths:\n",
    "    dt = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=10000, stop_words='english')),\n",
    "        ('clf', DecisionTreeClassifier(max_depth=depth, random_state=42))\n",
    "    ])\n",
    "    dt.fit(X_train, y_train)\n",
    "    train_scores.append(dt.score(X_train, y_train))\n",
    "    val_scores.append(dt.score(X_val, y_val))\n",
    "\n",
    "# Visualize the overfitting curve\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x_labels = [str(d) if d else 'None' for d in depths]\n",
    "x_pos = range(len(depths))\n",
    "\n",
    "ax.plot(x_pos, train_scores, 'o-', label='Train Accuracy', linewidth=2, markersize=8)\n",
    "ax.plot(x_pos, val_scores, 's-', label='Validation Accuracy', linewidth=2, markersize=8)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(x_labels)\n",
    "ax.set_xlabel('max_depth')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Overfitting: Train vs Validation Accuracy by Tree Depth')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight the gap\n",
    "best_val_idx = np.argmax(val_scores)\n",
    "ax.axvline(x=best_val_idx, color='green', linestyle='--', alpha=0.5,\n",
    "           label=f'Best validation (depth={x_labels[best_val_idx]})')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest validation accuracy: {max(val_scores):.3f} at max_depth={depths[best_val_idx]}\")\n",
    "print(f\"Unconstrained tree: train={train_scores[-1]:.3f}, val={val_scores[-1]:.3f}\")\n",
    "print(f\"\\nThe gap between train and validation curves IS overfitting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key insight:** As model complexity increases (deeper tree), training accuracy goes up but validation accuracy eventually goes down. The sweet spot is where the validation accuracy peaks.\n",
    "\n",
    "Compare this to Logistic Regression, which is simpler and doesn't overfit as easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression doesn't overfit here because it's a simpler model\n",
    "lr_train = model.score(X_train, y_train)\n",
    "lr_val = model.score(X_val, y_val)\n",
    "\n",
    "print(f\"Logistic Regression:\")\n",
    "print(f\"  Train accuracy:      {lr_train:.3f}\")\n",
    "print(f\"  Validation accuracy: {lr_val:.3f}\")\n",
    "print(f\"  Gap:                 {lr_train - lr_val:.3f}\")\n",
    "print(f\"\\nMuch smaller gap = less overfitting. Simpler models generalize better.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Save and Inspect the Model\n",
    "\n",
    "A trained model is just a collection of **learned parameters** (coefficients, thresholds, vocabulary mappings).\n",
    "We can save it to a file, load it later, and inspect what's inside.\n",
    "\n",
    "This file IS your model. Everything the model learned lives here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Save the trained pipeline (TF-IDF + Logistic Regression)\n",
    "model_path = 'sentiment_model.joblib'\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "size_bytes = os.path.getsize(model_path)\n",
    "size_mb = size_bytes / (1024 * 1024)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(f\"File size: {size_mb:.2f} MB ({size_bytes:,} bytes)\")\n",
    "print(f\"\\nThis file contains everything the model learned:\")\n",
    "print(f\"  - The TF-IDF vocabulary ({len(model.named_steps['tfidf'].vocabulary_):,} words)\")\n",
    "print(f\"  - The IDF weights for each word\")\n",
    "print(f\"  - The logistic regression coefficients\")\n",
    "print(f\"  - The intercept (bias) term\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load it back and verify it works\n",
    "loaded_model = joblib.load(model_path)\n",
    "\n",
    "# Test on a few examples\n",
    "test_texts = [\n",
    "    \"The pitcher threw a fastball and struck out the batter in the ninth inning.\",\n",
    "    \"NASA launched a new satellite to study the atmosphere of Mars.\",\n",
    "    \"The home run in the bottom of the seventh sealed the championship.\",\n",
    "    \"The telescope captured images of a distant galaxy cluster.\"\n",
    "]\n",
    "\n",
    "loaded_predictions = loaded_model.predict(test_texts)\n",
    "loaded_probabilities = loaded_model.predict_proba(test_texts)\n",
    "\n",
    "print(\"Predictions from the loaded model:\\n\")\n",
    "for text, pred, proba in zip(test_texts, loaded_predictions, loaded_probabilities):\n",
    "    label = train_data.target_names[pred]\n",
    "    confidence = max(proba) * 100\n",
    "    print(f\"  [{label:>20s}] ({confidence:.0f}% confident) \\\"{text[:60]}...\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the learned parameters: what words matter most?\n",
    "feature_names = model.named_steps['tfidf'].get_feature_names_out()\n",
    "coefficients = model.named_steps['clf'].coef_[0]\n",
    "\n",
    "# Sort by coefficient value\n",
    "sorted_idx = np.argsort(coefficients)\n",
    "\n",
    "n_top = 15\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Top words for class 0 (baseball) â€” most negative coefficients\n",
    "top_baseball = sorted_idx[:n_top]\n",
    "axes[0].barh(range(n_top), coefficients[top_baseball], color='#2196F3')\n",
    "axes[0].set_yticks(range(n_top))\n",
    "axes[0].set_yticklabels([feature_names[i] for i in top_baseball])\n",
    "axes[0].set_title(f'Top {n_top} words â†’ {train_data.target_names[0]}')\n",
    "axes[0].set_xlabel('Coefficient (more negative = stronger signal)')\n",
    "\n",
    "# Top words for class 1 (space) â€” most positive coefficients\n",
    "top_space = sorted_idx[-n_top:]\n",
    "axes[1].barh(range(n_top), coefficients[top_space], color='#FF9800')\n",
    "axes[1].set_yticks(range(n_top))\n",
    "axes[1].set_yticklabels([feature_names[i] for i in top_space])\n",
    "axes[1].set_title(f'Top {n_top} words â†’ {train_data.target_names[1]}')\n",
    "axes[1].set_xlabel('Coefficient (more positive = stronger signal)')\n",
    "\n",
    "plt.suptitle('What the Model Learned: Most Important Words per Class', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nThe model learned {len(feature_names):,} word-to-number mappings.\")\n",
    "print(f\"Each word has a coefficient: positive = space, negative = baseball.\")\n",
    "print(f\"These coefficients ARE the model. That's all there is to it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the saved model file\n",
    "os.remove(model_path)\n",
    "print(f\"Cleaned up {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "You just built a machine learning model from scratch. Here's what you now know:\n",
    "\n",
    "| Concept | What You Learned |\n",
    "|---------|------------------|\n",
    "| **A model** | Learned parameters (coefficients, weights) stored in a file |\n",
    "| **Training** | Feeding data through an algorithm to learn those parameters |\n",
    "| **Accuracy trap** | On imbalanced data, accuracy lies â€” use precision, recall, F1, confusion matrix |\n",
    "| **Train/val/test** | Three-way split prevents you from fooling yourself during tuning |\n",
    "| **Overfitting** | Memorizing training data (100% train, bad test) â€” fix with simpler models or constraints |\n",
    "| **Model file** | A serialized object containing vocabulary + learned weights â€” nothing magical |\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **ML Lab 02**, you'll take this saved model and deploy it behind a FastAPI endpoint â€” turning a file on disk into a live prediction service."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
