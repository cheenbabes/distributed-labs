{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ML Lab 10: Build a RAG System\n",
    "\n",
    "Everyone is building RAG systems, but most people treat them as black boxes. In this lab,\n",
    "you'll poke inside one: send queries, examine retrieved chunks, experiment with chunking\n",
    "strategies, measure retrieval quality, and discover exactly where RAG systems break."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Query the RAG System\n",
    "\n",
    "The RAG API is running at `http://localhost:8000`. Documents about distributed systems,\n",
    "ML basics, and Python tips have already been ingested. Let's send queries and examine\n",
    "what comes back: the retrieved chunks, the generated answer, and the latency breakdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "RAG_API = \"http://localhost:8000\"\n",
    "\n",
    "# Verify the API is running\n",
    "health = requests.get(f\"{RAG_API}/health\").json()\n",
    "print(f\"API status: {health}\")\n",
    "\n",
    "# Check how many document chunks are stored\n",
    "stats = requests.get(f\"{RAG_API}/collection/stats\").json()\n",
    "print(f\"Collection: {stats['name']}, Chunks stored: {stats['count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(question, n_results=3):\n",
    "    \"\"\"Send a question to the RAG API and display results.\"\"\"\n",
    "    resp = requests.post(f\"{RAG_API}/query\", json={\n",
    "        \"question\": question,\n",
    "        \"n_results\": n_results,\n",
    "        \"model\": \"tinyllama\"\n",
    "    })\n",
    "    result = resp.json()\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(f\"Answer: {result['answer'][:500]}\")\n",
    "    print(f\"\\n--- Timing ---\")\n",
    "    print(f\"  Retrieval: {result['retrieval_time_ms']:.1f} ms\")\n",
    "    print(f\"  Generation: {result['generation_time_ms']:.1f} ms\")\n",
    "    print(f\"  Total: {result['total_time_ms']:.1f} ms\")\n",
    "    print(f\"\\n--- Sources ({len(result['sources'])}) ---\")\n",
    "    for i, src in enumerate(result['sources']):\n",
    "        print(f\"  [{i+1}] {src['metadata'].get('source', 'unknown')}: {src['chunk'][:100]}...\")\n",
    "    print()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 1: A topic well-covered by the documents\n",
    "result_cap = query_rag(\"What is the CAP theorem and what tradeoffs does it describe?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 2: Another well-covered topic\n",
    "result_gd = query_rag(\"How does gradient descent work in machine learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 3: A topic that spans multiple documents\n",
    "result_cross = query_rag(\"What are ensemble methods and how do they relate to overfitting?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 4: Something NOT in the documents at all\n",
    "result_miss = query_rag(\"What is the current stock price of NVIDIA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "**What you should see:** For topics covered in the documents (CAP theorem, gradient descent),\n",
    "the system retrieves relevant chunks and generates a grounded answer. For the stock price\n",
    "question, the system retrieves unrelated chunks and either hallucinates or says it doesn't know.\n",
    "\n",
    "Notice how generation time dominates total latency -- retrieval is fast, LLM generation is slow.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Section 2: Chunking Strategies\n",
    "\n",
    "Chunk size is the single most impactful parameter in a RAG system. Too small, and chunks\n",
    "lose context. Too large, and chunks dilute the relevant information with irrelevant text.\n",
    "\n",
    "Let's re-ingest the same document with different chunk sizes and see how it affects retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the distributed systems document\n",
    "import urllib.request\n",
    "\n",
    "# We'll read the document from the local file system or fetch it\n",
    "# Since we're outside the container, let's read from the mounted path\n",
    "doc_text = \"\"\"\n",
    "The CAP theorem, formulated by Eric Brewer in 2000, states that a distributed data store \n",
    "can provide at most two out of three guarantees simultaneously: Consistency, Availability, \n",
    "and Partition Tolerance. In practice, since network partitions are unavoidable in distributed \n",
    "systems, the real choice is between consistency and availability during a partition.\n",
    "\n",
    "Consensus algorithms allow distributed systems to agree on a single value even when some \n",
    "nodes fail. Paxos uses a three-phase protocol: Prepare, Promise, and Accept. Raft was \n",
    "designed as an understandable alternative to Paxos, decomposing consensus into leader \n",
    "election, log replication, and safety.\n",
    "\n",
    "Eventual consistency is a model in which, if no new updates are made, eventually all \n",
    "accesses will return the last updated value. This is weaker than strong consistency but \n",
    "allows for higher availability and lower latency.\n",
    "\n",
    "Vector clocks track causality in distributed systems. Each node maintains a vector of \n",
    "logical timestamps. CRDTs are data structures that can be replicated and updated \n",
    "independently without coordination, guaranteed to converge.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Document length: {len(doc_text.split())} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: chunk text locally so we can inspect the chunks\n",
    "def chunk_text(text, chunk_size, overlap):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "# Compare chunk counts at different sizes\n",
    "for size in [50, 100, 200, 500]:\n",
    "    chunks = chunk_text(doc_text, chunk_size=size, overlap=max(10, size // 10))\n",
    "    print(f\"Chunk size {size:>4d} words -> {len(chunks):>2d} chunks, \"\n",
    "          f\"avg {sum(len(c.split()) for c in chunks) / len(chunks):.0f} words/chunk\")\n",
    "    print(f\"  First chunk preview: {chunks[0][:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest with different chunk sizes and compare retrieval\n",
    "test_question = \"What is the CAP theorem?\"\n",
    "chunk_sizes = [50, 100, 200, 500]\n",
    "\n",
    "print(f\"Question: {test_question}\\n\")\n",
    "\n",
    "for size in chunk_sizes:\n",
    "    # Ingest with this chunk size\n",
    "    resp = requests.post(f\"{RAG_API}/ingest\", json={\n",
    "        \"text\": doc_text,\n",
    "        \"metadata\": {\"source\": f\"test_chunk_{size}\"},\n",
    "        \"chunk_size\": size,\n",
    "        \"chunk_overlap\": max(10, size // 10)\n",
    "    })\n",
    "    ingest_result = resp.json()\n",
    "    \n",
    "    # Query\n",
    "    resp = requests.post(f\"{RAG_API}/query\", json={\n",
    "        \"question\": test_question,\n",
    "        \"n_results\": 2,\n",
    "        \"model\": \"tinyllama\"\n",
    "    })\n",
    "    result = resp.json()\n",
    "    \n",
    "    print(f\"--- Chunk size: {size} words ({ingest_result['chunks']} chunks) ---\")\n",
    "    print(f\"  Retrieval time: {result['retrieval_time_ms']:.1f} ms\")\n",
    "    for i, src in enumerate(result['sources']):\n",
    "        print(f\"  Source [{i+1}]: {src['chunk'][:120]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "**Key insight:** With very small chunks (50 words), each chunk lacks context -- you get a\n",
    "sentence fragment that mentions \"CAP theorem\" but not the full explanation. With very large\n",
    "chunks (500+ words), the relevant information is buried among unrelated content.\n",
    "\n",
    "The sweet spot for most text is 150-300 words per chunk with 10-20% overlap. This is why\n",
    "**chunk size is the most impactful parameter in RAG** -- more than the embedding model,\n",
    "more than the LLM, more than the number of retrieved chunks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Section 3: Evaluate Retrieval Quality\n",
    "\n",
    "The LLM can only generate good answers if the retrieval step finds the right chunks.\n",
    "Let's create golden question-answer pairs and measure retrieval precision: for each\n",
    "question, is the correct source chunk in the top-3 retrieved results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Golden test set: questions with the expected source document\n",
    "golden_qa = [\n",
    "    {\"question\": \"What is the CAP theorem?\",\n",
    "     \"expected_source\": \"distributed_systems.md\"},\n",
    "    {\"question\": \"How does the Raft consensus algorithm work?\",\n",
    "     \"expected_source\": \"distributed_systems.md\"},\n",
    "    {\"question\": \"What are CRDTs and how do they work?\",\n",
    "     \"expected_source\": \"distributed_systems.md\"},\n",
    "    {\"question\": \"What is consistent hashing used for?\",\n",
    "     \"expected_source\": \"distributed_systems.md\"},\n",
    "    {\"question\": \"What is the difference between supervised and unsupervised learning?\",\n",
    "     \"expected_source\": \"ml_basics.md\"},\n",
    "    {\"question\": \"What is the bias-variance tradeoff?\",\n",
    "     \"expected_source\": \"ml_basics.md\"},\n",
    "    {\"question\": \"How does cross-validation work?\",\n",
    "     \"expected_source\": \"ml_basics.md\"},\n",
    "    {\"question\": \"What are Python generators and when should you use them?\",\n",
    "     \"expected_source\": \"python_tips.md\"},\n",
    "    {\"question\": \"How do Python decorators work?\",\n",
    "     \"expected_source\": \"python_tips.md\"},\n",
    "    {\"question\": \"What is NumPy used for in Python?\",\n",
    "     \"expected_source\": \"python_tips.md\"},\n",
    "]\n",
    "\n",
    "print(f\"Golden test set: {len(golden_qa)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate retrieval precision@3\n",
    "correct = 0\n",
    "results_log = []\n",
    "\n",
    "for qa in golden_qa:\n",
    "    resp = requests.post(f\"{RAG_API}/query\", json={\n",
    "        \"question\": qa[\"question\"],\n",
    "        \"n_results\": 3,\n",
    "        \"model\": \"tinyllama\"\n",
    "    })\n",
    "    result = resp.json()\n",
    "    \n",
    "    # Check if any retrieved source matches the expected document\n",
    "    retrieved_sources = [s[\"metadata\"].get(\"source\", \"\") for s in result[\"sources\"]]\n",
    "    hit = qa[\"expected_source\"] in retrieved_sources\n",
    "    if hit:\n",
    "        correct += 1\n",
    "    \n",
    "    results_log.append({\n",
    "        \"question\": qa[\"question\"][:50],\n",
    "        \"expected\": qa[\"expected_source\"],\n",
    "        \"retrieved\": retrieved_sources,\n",
    "        \"hit\": hit\n",
    "    })\n",
    "    \n",
    "    status = \"HIT\" if hit else \"MISS\"\n",
    "    print(f\"[{status}] {qa['question'][:60]}\")\n",
    "    print(f\"       Expected: {qa['expected_source']}\")\n",
    "    print(f\"       Got: {retrieved_sources}\")\n",
    "    print()\n",
    "\n",
    "precision = correct / len(golden_qa)\n",
    "print(f\"{'=' * 50}\")\n",
    "print(f\"Retrieval Precision@3: {precision:.1%} ({correct}/{len(golden_qa)})\")\n",
    "print(f\"{'=' * 50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "questions = [r[\"question\"][:30] + \"...\" for r in results_log]\n",
    "hits = [1 if r[\"hit\"] else 0 for r in results_log]\n",
    "colors = [\"#4CAF50\" if h else \"#F44336\" for h in hits]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bars = ax.barh(range(len(questions)), hits, color=colors)\n",
    "ax.set_yticks(range(len(questions)))\n",
    "ax.set_yticklabels(questions, fontsize=9)\n",
    "ax.set_xlabel(\"Retrieved Correct Source (1=yes, 0=no)\")\n",
    "ax.set_title(f\"Retrieval Precision@3: {precision:.0%}\")\n",
    "ax.set_xlim(-0.1, 1.1)\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"retrieval_precision.png\", dpi=100, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved to retrieval_precision.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "**What you should see:** Most questions should retrieve the correct source document in the\n",
    "top 3 results. Questions that fail are usually those where the wording doesn't closely\n",
    "match the document text, or where the topic spans multiple documents.\n",
    "\n",
    "This metric -- retrieval precision -- is the most important metric to track in a RAG system.\n",
    "If retrieval is wrong, no amount of LLM quality can save the answer.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Section 4: Breaking the System\n",
    "\n",
    "RAG systems fail in interesting ways. Let's stress-test ours with adversarial inputs\n",
    "and see how it handles each failure mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failure mode 1: Question not in the documents at all\n",
    "print(\"=\" * 60)\n",
    "print(\"FAILURE MODE 1: Out-of-scope question\")\n",
    "print(\"=\" * 60)\n",
    "result = query_rag(\"What is the recipe for chocolate cake?\")\n",
    "print(\"Observation: The system retrieves whatever is 'closest' in vector space,\")\n",
    "print(\"even though none of the chunks are actually relevant.\")\n",
    "print(\"The LLM may hallucinate an answer or (if we're lucky) say it doesn't know.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failure mode 2: Ambiguous question\n",
    "print(\"=\" * 60)\n",
    "print(\"FAILURE MODE 2: Ambiguous question\")\n",
    "print(\"=\" * 60)\n",
    "result = query_rag(\"What is consistency?\")\n",
    "print(\"Observation: 'Consistency' appears in both distributed systems (CAP theorem)\")\n",
    "print(\"and ML (consistent models). The system may mix concepts from different domains.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failure mode 3: Very short query\n",
    "print(\"=\" * 60)\n",
    "print(\"FAILURE MODE 3: Very short query\")\n",
    "print(\"=\" * 60)\n",
    "result = query_rag(\"Raft\")\n",
    "print(\"Observation: Single-word queries have less semantic signal for retrieval.\")\n",
    "print(\"The embedding of 'Raft' may or may not match the chunk about Raft consensus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failure mode 4: Very long query\n",
    "print(\"=\" * 60)\n",
    "print(\"FAILURE MODE 4: Very long query\")\n",
    "print(\"=\" * 60)\n",
    "long_question = (\n",
    "    \"I am building a distributed system and I need to understand all the different \"\n",
    "    \"consistency models and how they relate to the CAP theorem and also I want to know \"\n",
    "    \"about consensus algorithms like Paxos and Raft and how they handle leader election \"\n",
    "    \"and log replication and also what about CRDTs and vector clocks and gossip protocols \"\n",
    "    \"and how do all of these things fit together in a real system?\"\n",
    ")\n",
    "result = query_rag(long_question)\n",
    "print(\"Observation: Long queries dilute the semantic signal. The embedding tries to\")\n",
    "print(\"represent too many concepts at once, and retrieval may miss the most relevant chunk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failure mode 5: Trick question (looks relevant but is adversarial)\n",
    "print(\"=\" * 60)\n",
    "print(\"FAILURE MODE 5: Trick question\")\n",
    "print(\"=\" * 60)\n",
    "result = query_rag(\"Why is the CAP theorem wrong and outdated?\")\n",
    "print(\"Observation: The documents describe CAP factually. But the question frames it\")\n",
    "print(\"as 'wrong'. The LLM might agree with the premise (hallucination) or correctly\")\n",
    "print(\"note that the documents don't say it's wrong.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "**Key takeaways from breaking the system:**\n",
    "\n",
    "1. **RAG systems always retrieve something** -- even when nothing is relevant. There is no built-in \"I don't know\" from the retrieval step.\n",
    "2. **Ambiguous queries retrieve mixed results** -- the system can't ask for clarification.\n",
    "3. **Query length affects retrieval** -- too short lacks signal, too long dilutes it.\n",
    "4. **The LLM trusts its context** -- if irrelevant chunks are retrieved, the LLM will try to use them.\n",
    "5. **Adversarial framing fools the LLM** -- the retrieval finds the right content, but the question's framing can lead to wrong answers.\n",
    "\n",
    "In production RAG systems, you need:\n",
    "- Retrieval score thresholds to filter out low-confidence results\n",
    "- Query classification to detect out-of-scope questions\n",
    "- Answer grounding checks to verify the answer is supported by the retrieved text\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've built and thoroughly tested a RAG system. Here's what you now know:\n",
    "\n",
    "| Concept | What You Learned |\n",
    "|---------|------------------|\n",
    "| **RAG Pipeline** | Ingest -> Chunk -> Embed -> Store -> Retrieve -> Generate |\n",
    "| **Chunk Size** | The most impactful parameter -- 150-300 words is the sweet spot |\n",
    "| **Retrieval Precision** | The metric that matters most -- bad retrieval means bad answers |\n",
    "| **Failure Modes** | RAG always retrieves something, even when nothing is relevant |\n",
    "| **Latency** | Retrieval is fast (ms), generation is slow (seconds) |\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **ML Lab 11**, you'll build an AI agent that uses this RAG system as one of its tools,\n",
    "along with a calculator, search, and time tool -- with guardrails to prevent misuse."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 2,
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
